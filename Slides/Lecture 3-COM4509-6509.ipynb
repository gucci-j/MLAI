{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression \n",
    "\n",
    "## Machine Learning and Adaptive Intelligence\n",
    "\n",
    "### Mauricio Álvarez \n",
    "\n",
    "### Based on slides by Neil D. Lawrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Atsuki/.pyenv/versions/3.6.2/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review\n",
    "- Last time: Looked at objective functions for movie recommendation.\n",
    "- Minimized sum of squares objective by steepest descent and stochastic gradients.\n",
    "- This time: explore least squares for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Examples\n",
    "\n",
    "-   Predict a real value, $y_i$ given some inputs\n",
    "    $x_i$.\n",
    "\n",
    "-   Predict quality of meat given spectral measurements (Tecator data).\n",
    "\n",
    "-   Radiocarbon dating, the C14 calibration curve: predict age given\n",
    "    quantity of C14 isotope.\n",
    "\n",
    "-   Predict quality of different Go or Backgammon moves given expert\n",
    "    rated training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "-  Gold medal times for Olympic 100 m runners since 1896.\n",
    "\n",
    "![image](./diagrams/100m_final_start.jpg)\n",
    "Image from Wikimedia Commons <http://bit.ly/191adDC>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic 100m Data\n",
    "\n",
    "\n",
    "<img src=\"diagrams/male100.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n",
    "\n",
    "-   Gold medal times for Olympic Marathon since 1896.\n",
    "\n",
    "-   Marathons before 1924 didn’t have a standardised distance.\n",
    "\n",
    "-   Present results using pace per km.\n",
    "\n",
    "-   In 1904 Marathon was badly organised leading to very slow times.\n",
    "\n",
    "<img src=\"diagrams/Eliud_Kipchoge.jpg\" width=\"300\" height=\"40\" align=center>\n",
    "\n",
    "Image from Wikimedia Commons [Eliud Kipchoge](https://commons.wikimedia.org/wiki/File:Eliud_Kipchoge_in_Berlin_-_2015_(cropped).jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Olympic Marathon Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x119868518>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119868eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pods.datasets.olympic_marathon_men()\n",
    "# Adding 2016 time\n",
    "np.append(data['X'], 2016)\n",
    "np.append(data['Y'], (2*60+8+(44/60))/42.195) # 2:08:44\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(data['X'], data['Y'], 'ro',markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "$$ \\text{data} + \\text{model} = \\text{prediction}$$\n",
    "\n",
    "-   $\\text{data}$ : observations, could be actively or passively\n",
    "    acquired (meta-data).\n",
    "\n",
    "-   $\\text{model}$ : assumptions, based on previous experience (other data!\n",
    "    transfer learning etc), or beliefs about the regularities of\n",
    "    the universe. Inductive bias.\n",
    "\n",
    "-   $\\text{prediction}$ : an action to be taken or a categorization or a\n",
    "    quality score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression: Linear Releationship\n",
    "\n",
    "$$y_i = m x_i + c$$\n",
    "\n",
    "-   $y_i$ : winning time/pace.\n",
    "\n",
    "-   $x_i$ : year of Olympics.\n",
    "\n",
    "-   $m$ : rate of improvement over time.\n",
    "\n",
    "-   $c$ : winning time at year 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overdetermined System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "\n",
    "*We know how to solve this system of two equations and two unknowns*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/two_points_plus_line.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We now observe a new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"diagrams/three_points.jpeg\" width=\"500\" height=\"40\" align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ $$3 = m + c$$ \n",
    "point 2: $x = 3$, $y=1$ $$1 = 3m + c$$ \n",
    "point 3: $x = 2$, $y=2.5$ $$2.5 = 2m + c$$\n",
    "\n",
    "*Overdetermined system (more equations than unknowns): three equations and two unknowns* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"diagrams/Pierre-Simon_Laplace.png\" align=center width=40%>\n",
    "\n",
    "[Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# $y = mx + c + \\epsilon$\n",
    "\n",
    "point 1: $x = 1$, $y=3$ \n",
    "$$3 = m + c + \\epsilon_1$$ \n",
    "\n",
    "point 2: $x = 3$, $y=1$ \n",
    "$$1 = 3m + c + \\epsilon_2$$ \n",
    "\n",
    "point 3: $x = 2$, $y=2.5$ \n",
    "$$2.5 = 2m + c + \\epsilon_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Density\n",
    "\n",
    "Perhaps the most common probability density.\n",
    "\n",
    "\\begin{align*}\n",
    "p(y| \\mu, \\sigma^2) & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\\\\\n",
    "                    & \\buildrel\\triangle\\over = \\mathcal{N}(y|\\mu, \\sigma^2)\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "\n",
    "![](./diagrams/gaussian_of_height.svg)\n",
    "\n",
    "The Gaussian PDF with $\\mu=1.7$ and variance $\\sigma^2=\n",
    "  0.0225$. Mean shown as red line. It could represent the heights of a population of\n",
    "  students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gaussian Density\n",
    "$$\n",
    "\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "$\\sigma^2$ is the variance of the density and $\\mu$ is the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The IID Assumption\n",
    "\n",
    "- IID stands for Independent and Identically Distributed\n",
    "\n",
    "\n",
    "- We say that a set of random variables are IID if they are statistical independent AND each of them follows the same distribution.\n",
    "\n",
    "\n",
    "- Remember that a set of random variables is statistical independent if their joint distribution is equal to the product of their marginal distributions, $P(X, Y, Z) = P(X)P(Y)P(Z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Sum of Gaussian**\n",
    "\n",
    "-   Sum of (i.i.d) Gaussian variables is also Gaussian.\n",
    "    \n",
    "    $$y_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2_i)$$ \n",
    "    \n",
    "    And the sum is distributed as\n",
    "    \n",
    "    $$\\sum_{i=1}^{n} y_i \\sim \\mathcal{N}\\left(\\sum_{i=1}^n \\mu_i,\\sum_{i=1}^n \\sigma_i^2\\right)$$\n",
    "    \n",
    "    (*Aside*: As sum increases, sum of non-Gaussian, finite variance variables is\n",
    "    also Gaussian [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two Important Gaussian Properties\n",
    "\n",
    "**Scaling a Gaussian**\n",
    "\n",
    "-   Scaling a Gaussian leads to a Gaussian.\n",
    "    \n",
    "    $$y \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "    \n",
    "    And the scaled density is distributed as\n",
    "    \n",
    "    $$w y \\sim \\mathcal{N}(w\\mu,w^2 \\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A probabilistic interpretation of the observations\n",
    "\n",
    "-   Set the mean of Gaussian to be a function.\n",
    "    \n",
    "    $$p\\left(y_i|x_i\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-f\\left(x_i\\right)\\right)^{2}}\n",
    "    {2\\sigma^2}\\right).$$\n",
    "\n",
    "\n",
    "-   This gives us a ‘noisy function’.\n",
    "\n",
    "- can be used as generative models（シグマを指定すれば良い）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Height as a Function of Weight\n",
    "\n",
    "-   In the standard Gaussian, parametized by mean and variance, make the mean a linear function of an *input*.\n",
    "\n",
    "-   This leads to a regression model. \n",
    "\n",
    "    \\begin{align*}\n",
    "               y_i  =  & \\,f\\left(x_i\\right)+\\epsilon_i,\\\\\n",
    "       \\epsilon_i \\sim &  \\mathcal{N}(0, \\sigma^2).\n",
    "     \\end{align*}\n",
    "        \n",
    "-   Assume $y_i$ is height and $x_i$ is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Point Likelihood\n",
    "\n",
    "-   Likelihood of an individual data point\n",
    "    $$p\\left(y_i|x_i,m,c, \\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n",
    "             \\exp \\left(-\\frac{\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "    \n",
    "\n",
    "-   Parameters are gradient, $m$, offset, $c$ of the function and noise\n",
    "    variance $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Set Likelihood\n",
    "\n",
    "-   If the noise, $\\epsilon_i$ is sampled independently for each\n",
    "    data point.\n",
    "\n",
    "-   Each data point is independent (given $m$ and $c$).\n",
    "\n",
    "-   Define \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{y} =\n",
    "                \\begin{bmatrix}\n",
    "                    y_1\\\\\n",
    "                    y_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    y_n\n",
    "                \\end{bmatrix},\n",
    "     \\quad\n",
    "     \\mathbf{x} =\n",
    "                \\begin{bmatrix}\n",
    "                    x_1\\\\\n",
    "                    x_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    x_n\n",
    "                \\end{bmatrix},\n",
    "\\end{align*}\n",
    "\n",
    "-   For independent variables:\n",
    "    $$p(\\mathbf{y}) = \\prod_{i=1}^n p(y_i)$$\n",
    "    $$p(\\mathbf{y}|\\mathbf{x}, m, c,\\sigma^2) = \\prod_{i=1}^n p(y_i|x_i, m, c, \\sigma^2)$$\n",
    "    \n",
    "    m, c, シグマを定めたら，各データは独立（iid）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For Gaussian \n",
    "\n",
    "- i.i.d. assumption\n",
    "\\begin{align*}\n",
    "                   p(\\mathbf{y}) &= \\prod_{i=1}^n p(y_i)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c, \\sigma^2)&= \\prod_{i=1}^n p(y_i|x_i, m, c, \\sigma^2)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c, \\sigma^2)&= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp \\left(-\\frac{\\left(y_i-mx_i-  \n",
    "                                     c\\right)^{2}}{2\\sigma^2}\\right)\\\\\n",
    "   p(\\mathbf{y}|\\mathbf{x}, m, c, \\sigma^2)&= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n",
    "                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right).\n",
    " \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood Function\n",
    "\n",
    "- We can use the likelihood function, $p(\\mathbf{y}|\\mathbf{x}, m, c)$, \n",
    "\n",
    "     $$p(\\mathbf{y}|\\mathbf{x}, m, c)= \\frac{1}{\\left(2\\pi \\sigma^2\\right)^{\\frac{n}{2}}}\\exp \\left(-  \n",
    "                                      \\frac{\\sum_{i=1}^n\\left(y_i-mx_i-c\\right)^{2}}{2\\sigma^2}\\right),$$\n",
    "\n",
    " to estimate the parameters $m$, $c$ and $\\sigma^2$.\n",
    "\n",
    "- In practice, we prefer to work with the log likelihood:\n",
    "    \n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\sum _{i=1}^{n}\\frac{\\left(y_i-mx_i-    \n",
    "        c\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consistency of Maximum Likelihood\n",
    "\n",
    "\n",
    "-   If data was really generated according to the probability we specified, the correct parameters will be recovered   in the limit as $n \\rightarrow \\infty$.\n",
    "\n",
    "\n",
    "-   This can be proven through sample based approximations (law of large numbers) of “KL divergences”.\n",
    "\n",
    "\n",
    "-   Mainstay of classical statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Probabilistic Interpretation of the Error Function\n",
    "\n",
    "-   Probabilistic Interpretation for Error Function is Negative Log Likelihood.\n",
    "\n",
    "\n",
    "-   *Minimizing* error function is equivalent to *maximizing* log likelihood.\n",
    "\n",
    "\n",
    "-   Maximizing *log likelihood* is equivalent to maximizing the *likelihood* because $\\log$ is monotonic. (monotonic: 単調関数)\n",
    "\n",
    "\n",
    "-   Probabilistic interpretation: Minimizing error function is equivalent to maximum likelihood with respect to   parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error Function\n",
    "\n",
    "- Remember the expression for the log likelihood function:\n",
    "    \n",
    "    $$L(m,c,\\sigma^{2})=-\\frac{n}{2}\\log 2\\pi -\\frac{n}{2}\\log \\sigma^2 -\\frac{1}\n",
    "    {2\\sigma^2}\\sum_{i=1}^{n}\\left(y_i-mx_i-c\\right)^{2}.$$\n",
    "\n",
    "-  The negative log likelihood is the error function \n",
    "\n",
    "    $$E(m,c,\\sigma^{2})=\\frac{n}{2}\\log \\sigma^2 +\\frac{1}{2\\sigma^2}\\sum _{i=1}^{n}\\left(y_i-         mx_i-c\\right)^{2},$$\n",
    "    \n",
    "   where we have omitted the term $\\frac{n}{2}\\log 2\\pi$ that does not depend on the   parameters.\n",
    "\n",
    "-   Learning proceeds by minimizing this error function for the data\n",
    "    set provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connection: Sum of Squares Error\n",
    "\n",
    "-   Ignoring terms which don’t depend on $m$ and $c$ gives\n",
    "    \n",
    "    $$E(m, c) \\propto \\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "    \n",
    "    where $f(x_i) = mx_i + c$.\n",
    "\n",
    "\n",
    "-   This is known as the *sum of squares* error function.\n",
    "\n",
    "\n",
    "-   Commonly used and it is closely associated with the Gaussian likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder\n",
    "\n",
    "- Two functions involved:\n",
    "\n",
    "  - Prediction function: $f(x_i)$\n",
    "\n",
    "  - Error, or Objective function: $E(m, c)$\n",
    "\n",
    "\n",
    "- Error function depends on parameters through prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Interpretation\n",
    "\n",
    "-   What is the mathematical interpretation?\n",
    "\n",
    "    -   There is a cost function.\n",
    "\n",
    "    -   It expresses mismatch between your prediction and reality.\n",
    "        $$E(m, c)=\\sum_{i=1}^n \\left(y_i - mx_i -c\\right)^2$$\n",
    "\n",
    "    -   This is known as the sum of squares error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning consists of minimizing the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-   **Coordinate ascent**: find gradient in each coordinate (座標) and set to zero.\n",
    "    \\begin{align}\n",
    "     \\frac{\\text{d}E(m)}{\\text{d}m} &= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\\\\\n",
    "                                   0&= -2\\sum_{i=1}^n x_i\\left(y_i- m x_i - c \\right)\n",
    "    \\end{align}                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equation for $m$\n",
    "    $$0 =\n",
    "          -2\\sum_{i=1}^n x_iy_i\n",
    "          +2\\sum_{i=1}^n\n",
    "            m x_i^2 +2\\sum_{i=1}^n cx_i$$\n",
    "    $$m  =    \\frac{\\sum_{i=1}^n \\left(y_i\n",
    "          -c\\right)x_i}{\\sum_{i=1}^nx_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "-   Learning consists of minimizing the cost function.\n",
    "\n",
    "-   At the minima the gradient is zero.\n",
    "\n",
    "-  **Coordinate ascent**: find gradient in each coordinate and set to zero.\n",
    "     $$\\frac{\\text{d}E(c)}{\\text{d}c} = -2\\sum_{i=1}^n \\left(y_i- m x_i - c \\right)$$\n",
    "    \n",
    "     $$0 = -2\\sum_{i=1}^n\\left(y_i-m x_i - c \\right)$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning is Optimization\n",
    "\n",
    "- Fixed point equation for $c$\n",
    "    \\begin{align}\n",
    "        0 &= -2\\sum_{i=1}^n y_i +2\\sum_{i=1}^n m x_i +2n c\\\\\n",
    "        c &= \\frac{\\sum_{i=1}^n \\left(y_i-mx_i\\right)}{n}\n",
    "    \\end{align}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fixed Point Updates\n",
    "\n",
    "Worked example. \n",
    "   \n",
    "   \\begin{align}\n",
    "       c^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i\\right)}{n},\\\\\n",
    "       m^{*}=&\\frac{\\sum _{i=1}^{n}x_i\\left(y_i-c^{*}\\right)}{\\sum _{i=1}^{n}x_i^{2}},\\\\\n",
    "      \\left.\\sigma^2\\right.^{*}=&\\frac{\\sum _{i=1}^{n}\\left(y_i-m^{*}x_i-c^{*}\\right)^{2}}{n}\n",
    "  \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Concepts Not Covered\n",
    "\n",
    "-   Other optimization methods:\n",
    "\n",
    "    -   Second order methods, conjugate gradient, quasi-Newton\n",
    "        and Newton.\n",
    "\n",
    "    -   Effective heuristics such as momentum.\n",
    "\n",
    "-   Local vs global solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.1-1.2 of Rogers and Girolami (2016) for fitting linear models. \n",
    "- Section 1.2.5 of Bishop (2006) up to equation 1.65.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-dimensional Inputs\n",
    "\n",
    "-   Multivariate functions involve more than one input.\n",
    "\n",
    "-   Height might be a function of weight and gender.\n",
    "\n",
    "-   There could be other contributory factors.\n",
    "\n",
    "-   Place these factors in a feature vector $\\mathbf{x}_i$.\n",
    "\n",
    "-   Linear function is now defined as\n",
    "    $$f(\\mathbf{x}_i) = \\sum_{j=1}^D w_j x_{i, j} + c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Notation\n",
    "\n",
    "-   Write in vector notation,\n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + c$$\n",
    "    \n",
    "    \n",
    "\n",
    "-   **We can absorb $c$ into $\\mathbf{w}$ by assuming extra input $x_0$\n",
    "    which is always 1**. \n",
    "    \n",
    "    $$f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log Likelihood for Multivariate Regression\n",
    "\n",
    "-   The likelihood of a single data point is\n",
    "    $$p\\left(y_i|x_i, \\mathbf{w},\\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\n",
    "        \\left(-\\frac{\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}\\right).$$\n",
    "\n",
    "-   Leading to a log likelihood for the data set of\n",
    "    $$L(\\mathbf{w},\\sigma^2)= -\\frac{n}{2}\\log \\sigma^2\n",
    "          -\\frac{n}{2}\\log 2\\pi -\\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$\n",
    "\n",
    "-   And a corresponding error function of\n",
    "    $$E(\\mathbf{w},\\sigma^2)= \\frac{n}{2}\\log\n",
    "          \\sigma^2 + \\frac{\\sum\n",
    "            _{i=1}^{n}\\left(y_i-\\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expand the Brackets\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = & \\frac{n}{2}\\log\\sigma^2 + \\frac{\\sum_{i=1}^{n}\\left(y_i- \n",
    "                               \\mathbf{w}^{\\top}\\mathbf{x}_i\\right)^{2}}{2\\sigma^2}.\\\\\n",
    "                          = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum           \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i\\mathbf{w}^{\\top}\\mathbf{x}_i+\\frac{1}\n",
    "                              {2\\sigma^2}\\sum_{i=1}^{n}                            \n",
    "                              \\mathbf{w}^{\\top}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\mathbf{w}.\\\\\n",
    "                         = & \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n",
    "                             +\\frac{1}{2\\sigma^2} \n",
    "                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n",
    "                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w}.\n",
    "\\end{align*}\n",
    "\n",
    "$\\mathbf{w}^T \\mathbf{x}_i = \\mathbf{x}_i^T \\mathbf{w}$を使った\n",
    "**最後の導出の部分がよくわからない**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Derivatives\n",
    "\n",
    "-   We will need some matrix calculus.\n",
    "\n",
    "\n",
    "-   For now some simple multivariate differentiation:\n",
    "    $$\\frac{\\text{d}{\\mathbf{w}^{\\top}}{\\mathbf{a}}}{\\text{d}\\mathbf{w}}=\\mathbf{a}$$\n",
    "    and\n",
    "    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=\\left(\\mathbf{A}+\\mathbf{A}^{\\top}\\right)\\mathbf{w}$$\n",
    "    or if $\\mathbf{A}$ is symmetric (*i.e.*\n",
    "    $\\mathbf{A}=\\mathbf{A}^{\\top}$)\n",
    "    $$\\frac{\\text{d}\\mathbf{w}^{\\top}\\mathbf{A}\\mathbf{w}}{\\text{d}\\mathbf{w}}=2\\mathbf{A}\\mathbf{w}.$$\n",
    "    \n",
    "    \n",
    "- A good reference for matrix calculus is \"The Matrix Cookbook\" by K B Petersen and M S Pedersen (2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Differentiate\n",
    "\n",
    "The objective function is given as\n",
    "\n",
    "\\begin{align*}\n",
    "  E(\\mathbf{w},\\sigma^2)  = \\frac{n}{2}\\log \\sigma^2 + \\frac{1}{2\\sigma^2}\\sum \n",
    "                              _{i=1}^{n}y_i^{2}-\\frac{1}{\\sigma^2}\n",
    "                             \\mathbf{w}^\\top\\sum_{i=1}^{n}\\mathbf{x}_iy_i\n",
    "                             +\\frac{1}{2\\sigma^2} \n",
    "                           \\mathbf{w}^{\\top}\\left[\\sum_{i=1}^{n}\\mathbf{x}_i\n",
    "                           \\mathbf{x}_i^{\\top}\\right]\\mathbf{w}.\n",
    "\\end{align*}\n",
    "\n",
    "Differentiating with respect to the vector $\\mathbf{w}$ we obtain\n",
    "\n",
    "$$\\frac{\\partial E\\left(\\mathbf{w},\\sigma^2 \\right)}{\\partial \\mathbf{w}}=-\\frac{1}{\\sigma^2} \\sum _{i=1}^{n}\\mathbf{x}_iy_i+\\frac{1}{\\sigma^2} \\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]\\mathbf{w}$$\n",
    "\n",
    "Leading to\n",
    "$$\\mathbf{w}^{*}=\\left[\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^{\\top}\\right]^{-1}\\sum _{i=1}^{n}\\mathbf{x}_iy_i.$$\n",
    "\n",
    "Using matrix notation, it can be shown that:\n",
    "$$\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top = \\mathbf{X}^\\top \\mathbf{X}\\qquad \\sum _{i=1}^{n}\\mathbf{x}_iy_i = \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top=\\mathbf{X}^{\\top}\\mathbf{X}$\n",
    "\n",
    "Let us write \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}^{\\top}_1\\\\\n",
    "                    \\mathbf{x}^{\\top}_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    \\mathbf{x}^{\\top}_n\n",
    "                \\end{bmatrix},\n",
    "     \\quad\n",
    "     \\mathbf{X}^{\\top} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "We can then say that\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X}^{\\top}\\mathbf{X} =\n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}_1\\; \\mathbf{x}_2\\; \\cdots \\; \\mathbf{x}_n\\\\               \n",
    "                \\end{bmatrix} \n",
    "                \\begin{bmatrix}\n",
    "                    \\mathbf{x}^{\\top}_1\\\\\n",
    "                    \\mathbf{x}^{\\top}_2\\\\\n",
    "                    \\vdots\\\\\n",
    "                    \\mathbf{x}^{\\top}_n\n",
    "                \\end{bmatrix} =  \n",
    "                \\mathbf{x}_1\\mathbf{x}^{\\top}_1+\\mathbf{x}_2\\mathbf{x}^{\\top}_2+\\cdots+\\mathbf{x}_n\n",
    "                \\mathbf{x}^{\\top}_n = \\sum _{i=1}^{n}\\mathbf{x}_i\\mathbf{x}_i^\\top.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Update Equations\n",
    "\n",
    "-   Update for $\\mathbf{w}^{*}$.\n",
    "    $$\\mathbf{w}^{*} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "\n",
    "-   The solution for $\\mathbf{w}^{*}$ exists if we can compute $\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1}$. The    inverse can be computed as long as $\\mathbf{X}^\\top \\mathbf{X}$ is non-singular (e.g. determinat different from zero, or has full-rank).\n",
    "\n",
    "\n",
    "-   The equation for $\\left.\\sigma^2\\right.^{*}$ may also be found\n",
    "    $$\\left.\\sigma^2\\right.^{{*}}=\\frac{\\sum _{i=1}^{n}\\left(y_i-\\left.\\mathbf{w}^{*}\\right.^{\\top}\\mathbf{x}_i\\right)^{2}}{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading\n",
    "\n",
    "- Section 1.3 of Rogers and Girolami (2016) for Matrix & Vector Review.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

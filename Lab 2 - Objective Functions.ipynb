{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Functions: A Simple Example with Matrix Factorisation.\n",
    "\n",
    "### Modified by Mauricio A Álvarez, October 2018, 2019\n",
    "\n",
    "### 6th October 2015 Neil D. Lawrence\n",
    "\n",
    "In last week's class we saw how we could load in a data set to pandas and use it for some simple data processing. We computed various probabilities on the data and I encouraged you to think about what sort of probabilities you need for prediction. This week we are going to take a slightly different tack. \n",
    "\n",
    "Broadly speaking there are two dominating approaches to machine learning problems. We started to consider the first approach last week: constructing models based on defining the relationship between variables using probabilities. This week we will consider the second approach: which involves defining an *objective function* and optimizing it. \n",
    "\n",
    "What do we mean by an objective function? An objective function could be an *error function*, a *cost function* or a *benefit* function. In evolutionary computing they are called *fitness* functions. But the idea is always the same. We write down a mathematical equation which is then optimized to do the learning. The equation should be a function of the *data* and our model *parameters*. We have a choice when optimizing, either minimize or maximize. To avoid confusion, in the optimization field, we always choose to minimize the function. If we have a function that we would like to maximize, we simply choose to minimize the negative of that function. \n",
    "\n",
    "So for this lab session, we are going to ignore probabilities, but don't worry, they will return! \n",
    "\n",
    "This week we are going to try and build a simple movie recommender system using an objective function. To do this, the first thing I'd like you to do is to install some software we've written for sharing information across google documents.\n",
    "\n",
    "## Open Data Science Software\n",
    "\n",
    "In Sheffield we have written a suite of software tools for 'Open Data Science'. Open data science is an approach to sharing code, models and data that should make it easier for companies, health professionals and scientists to gain access to data science techniques. For some background on open data science you can read [this blog post](http://inverseprobability.com/2014/07/01/open-data-science/). The first thing we will do this week is to download that suite of software. \n",
    "\n",
    "The software can be installed using\n",
    "\n",
    "```python\n",
    "pip install pods\n",
    "```\n",
    "\n",
    "from the command prompt where you can access your python installation.\n",
    "\n",
    "\n",
    "## Download the MovieLens 100k Data\n",
    "\n",
    "We are going to download the [MovieLens 100k](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html) Data. This is a public dataset that contains 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. When you use a data set that someone has prepared you should always reference the data source to acknowledge the work that's been placed in. This particular dataset was collected by the [Grouplens Research group](https://grouplens.org/),  at the University of Minnesota. For example, if you were to use this dataset for writing a paper, the authors ask you that you acknowledge their work by citing the following paper:\n",
    "\n",
    "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5 (4):1-19 [https://doi.org/10.1145/2827872](https://doi.org/10.1145/2827872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Atsuki/.pyenv/versions/3.6.2/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pods\n",
    "import zipfile\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Atsuki/.pyenv/versions/3.6.2/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading  http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -> ./ml-latest-small.zip\n",
      "[==============================]   0.933/0.933MB                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "pods.util.download_url(\"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\")\n",
    "zip_console = zipfile.ZipFile('ml-latest-small.zip', 'r')\n",
    "for name in zip_console.namelist():\n",
    "           zip_console.extract(name, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Data ethics. If you find data available on the internet, can you simply use it without consequence? If you are given data by a fellow researcher can you publish that data on line? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer\n",
    "\n",
    "We have to check the license of the data before use it.\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "A recommender system aims to make suggestions for items (films, books, other commercial products) given what it knows about users' tastes. The recommendation engine needs to represent the *taste* of all the users and the *characteristics* of each object. \n",
    "\n",
    "A common way for organizing objects is to place related objects spatially close together. For example in a library we try and put books that are on related topics near to each other on the shelves. One system for doing this is known as [Dewey Decimal Classification](http://en.wikipedia.org/wiki/Dewey_Decimal_Classification). In the Dewey Decimal Classification system (which dates from 1876) each subject is given a number (in fact it's a decimal number). For example, the field of Natural Sciences and Mathematics is given numbers which start with 500. Subjects based on Computer Science are given numbers which start 004 and works on the 'mathematical principles' of Computer science are given the series 004.0151 (which we might store as 4.0151 on a Computer). Whilst it's a classification system, the books in the library are typically laid out in the same order as the numbers, so we might expect that neighbouring numbers represent books that are related in subject. That seems to be exactly what we want when also representing films. Could we somehow represent each film's subject according to a number? In a similar way we could then imagine representing users with a list of numbers that represent things that each user is interested in.\n",
    "\n",
    "Actually a one dimensional representation of a subject can be very awkward. To see this, let's have a look at the Dewey Decimal Classification numbers for the 900s, which is listed as 'History and Geography'. We will focus on subjects in the 940s which can be found in this list from [Wikipedia](https://en.wikipedia.org/wiki/List_of_Dewey_Decimal_classes#Class_900_%E2%80%93_History_&_geography). Whilst the ordering for places is somewhat sensible, it is also rather arbitrary. In the 940s we have Europe listed from 940-949, Asia listed from 950-959 and Africa listed from 960-969. Whilst it's true that Asia borders Europe, Africa is also very close, and the history of the Roman Empire spreads into [Carthage](http://en.wikipedia.org/wiki/Carthage) and later on Egypt. This image from Wikipedia shows a map of the Cathaginian Empire which fell after fighting with Rome. \n",
    "\n",
    "\n",
    "<a title=\"By Javierfv1212 [Public domain], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Carthaginianempire.PNG\"><img width=\"512\" alt=\"Carthaginianempire\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/Carthaginianempire.PNG/512px-Carthaginianempire.PNG\"></a>\n",
    "\n",
    "We now need to make a decision about whether Roman Histories are European or African, ideally we'd like them to be somewhere between the two, but we can't place them there in the Dewey Decimal system because between Europe and Africa is Asia, which has less to do with the Roman Empire than either Europe or Africa. Of course the fact that we've used a map provides a clue as to what to do next. Libraries are actually laid out on floors, so what if we were to use the spatial lay out to organise the sujbects of the books in two dimensions. Books on Geography could be laid out according to where in the world they are referring to. \n",
    "\n",
    "Such complexities are very hard to encapsulate in one number, but inspired by the map examples we can start considering how we might lay out films in two dimensions. Similarly, we can consider laying out a map of people's interests. If the two maps correspond to one another, the map of people could reflect where they might want to live in 'subject space'. We can think of representing people's tastes as where they might best like to sit in the library to access easily the books they are most interested in.\n",
    "\n",
    "\n",
    "## Inner Products for Representing Similarity\n",
    "\n",
    "Ideas like the above are good for gaining intuitions about what we might want, but the one of the skills of data science is representing those ideas mathematically. Mathematical abstraction of a problem is one of the key ways in which we've been able to progress as a society. Understanding planetary motions, as well as those of the smallest molecule (to quote Laplace's [Philosophical Essay on Probabilities](http://books.google.co.uk/books?id=1YQPAAAAQAAJ&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false)) needed to be done mathematically. The right mathematical model in machine learning can be slightly more elusive, because constructing it is a two stage process. \n",
    "\n",
    "1. We have to determine the right intuition for the system we want to represent. Notions such as 'subject' and 'interest' are not mathematically well defined, and even when we create a new interpretation of what they might mean, each interpretation may have its own weaknesses. \n",
    "\n",
    "2. Once we have our interpretation we can attempt to mathematically formalize it. In our library interpretation, that's what we need to do next. \n",
    "\n",
    "### The Library on an Infinite Plane\n",
    "\n",
    "Let's imagine a library which stores all the items  we are interested in, not just books, but films and shopping items too. Such a library is likely to be very large, so we'll create it on an infinite two dimensional plane. This means we can use all the real numbers to represent the location of each item on the plane. For a two dimensional plane, we need to store the locations in a vector of numbers: we can decide that the $j$th item's location in the library is given by \n",
    "$$\n",
    "\\mathbf{v}_j = \\begin{bmatrix} v_{j,1} \\\\ v_{j,2}\\end{bmatrix},\n",
    "$$\n",
    "where $v_{j,1}$ represents the $j$th item's location in the East-West direction (or the $x$-axis) and $v_{j,2}$ represents the $j$th item's location in the North-South direction (or the $y$-axis). Now we need to specify the location where each user sits so that all the items that interest them are nearby: we can also represent the $i$th user's location with a vector \n",
    "$$\n",
    "\\mathbf{u}_i = \\begin{bmatrix} u_{i,1} \\\\ u_{i,2}\\end{bmatrix}.\n",
    "$$\n",
    "Finally, we need some way of recording a given user's affinity for a given item. This affinity might be the rating that the user gives the film. We can use $y_{i,j}$ to represent user $i$'s affinity for item $j$. \n",
    "\n",
    "For our film example we might imagine wanting to order films in a few ways. We could imagine organising films in the North-South direction as to how romantic they are. We could place the more romantic films further North and the less romantic films further South. For the East-West direction we could imagine ordering them according to how historic they are: we can imagine placing science fiction films to the East and historical drama to the West. In this case, fans of historical romances would be based in the North-West location, whilst fans of Science Fiction Action films might be located in the South-East (if we assume that 'Action' is the opposite of 'Romance', which is not necessarily the case). How do we lay out all these films? Have we got the right axes? In machine learning the answer is to 'let the data speak'. Use the data to try and obtain such a lay out. To do this we first need to obtain the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "As mentioned before, the MovieLens dataset that we'll use has 100,000 ratings to 9,000 movies by 600 users. For now, we will only work with a subset of the dataset. In particular, we will randomly chose a particular number of users and extract the movies and ratings that the users gave to those movies. Read the code below and understand what it is doing.\n",
    "\n",
    "**Before you run the code**, notice that `YourStudentID` in the first line is a variable that will specify the seed for the random number generator that will select a particular set of `nUsersInExample` users. Change the number that has been assigned by default to `YourStudentID` to the last three digits of your UCard number. All of you will have a different subset of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100836.000000</td>\n",
       "      <td>100836.000000</td>\n",
       "      <td>100836.000000</td>\n",
       "      <td>1.008360e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>326.127564</td>\n",
       "      <td>19435.295718</td>\n",
       "      <td>3.501557</td>\n",
       "      <td>1.205946e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>182.618491</td>\n",
       "      <td>35530.987199</td>\n",
       "      <td>1.042529</td>\n",
       "      <td>2.162610e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.281246e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>177.000000</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.019124e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>2991.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.186087e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.000000</td>\n",
       "      <td>8122.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.435994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>193609.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.537799e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              userId        movieId         rating     timestamp\n",
       "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
       "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
       "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
       "min         1.000000       1.000000       0.500000  8.281246e+08\n",
       "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
       "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
       "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
       "max       610.000000  193609.000000       5.000000  1.537799e+09"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YourStudentID = 1713928  # Include here the last three digits of your UCard number\n",
    "nUsersInExample = 10 # The maximum number of Users we're going to analyse at one time\n",
    "\n",
    "ratings = pd.read_csv(\"./ml-latest-small/ratings.csv\") \n",
    "\"\"\"\n",
    "ratings is a DataFrame with four columns: userId, movieId, rating and tags. We\n",
    "first want to identify how many unique users there are. We can use the unique \n",
    "method in pandas\n",
    "\"\"\"\n",
    "ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
      " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396\n",
      " 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468\n",
      " 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486\n",
      " 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504\n",
      " 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522\n",
      " 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540\n",
      " 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558\n",
      " 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576\n",
      " 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594\n",
      " 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610]\n"
     ]
    }
   ],
   "source": [
    "indexes_unique_users = ratings['userId'].unique()\n",
    "print(indexes_unique_users)\n",
    "n_users = indexes_unique_users.shape[0] # 610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[523 279 366 321 576  86 234 470 211 608 419 544 342 156 317 235 277 231\n",
      " 529 432 497 495 209 123 312 105 450 278 472 153 592 224 151 563 547 453\n",
      " 437 362  96 225  58 564 436 512 578 199  88 591 393 565 485 139 341 147\n",
      " 496 245 584 178 498 599 430 344 240 254 441 422 397 574 116 323 162 447\n",
      "  79 347 179 301  68 287 328 273 421 293 452 127 133 598 513 552 260   8\n",
      " 271 138 426 155 340 255  84  77   5 532 533 268 174  97 192 308  38 396\n",
      " 144 303 355 220 604 359  14   9 263 568 238 381 457 555 493  98 480 535\n",
      " 461 318 336  16 376 390  91 516 276 395 114 291  53 188 449 198 125 387\n",
      " 483 402  42 298 489 182 161   7  61  59 462   6 324 264 284 524 594 580\n",
      " 443 404 589 243  24 351 375 569 124 274 140 479  20 221 455 297 380 502\n",
      " 499  28 418 338 190 343 384 345 583 167 252 534 183 415  44 241  39 313\n",
      " 148 391 212 606 128 145 492 306 582 382 159 388  41 248 157 605 129  11\n",
      " 494 525 310  13 379 266 325 536 484 515 141 572 180  76 518 500 562 567\n",
      " 508 195 433 204 285 548 416  78 490 551 168 251 531 424 370  37  23 111\n",
      " 131 514  92 206  50 540 486 458 215 603 119  85 361 586  80   3 207 281\n",
      " 197 431 374  18  36 475 102 134 170  45 530 214  94 403 556 305 442 542\n",
      " 130 210 414 408 216 435 265 522 406 354 577 229   4 570 541 476 261 200\n",
      " 392  40 186 171  55  33  65 581 203 106  32 275  87  63 602 456 511 193\n",
      " 227 410 357 546 378 110 267 413 460 101 601  69 331 329  29 368 349  47\n",
      " 405 304 191 230 427 112 466  93  57 579 232 487 176 289 158 528 385 575\n",
      "  17 239 246  60  26 358 553 501 228 152 503 316  12 509 257 184 557 165\n",
      "  81 175 294 538 218 283  56 142 315 488 300 451  74 407 600  30 320  15\n",
      " 117 558 202 504 597 181 256 236 527 150 440 259 478 346 593 412 550 311\n",
      " 164 269 561 539 333 137 288 185  62 471 510  83 543  99 233 314  89 377\n",
      " 372 307 517  82 163 222  72 177 411 507 386 330 607 282 353  67  70 373\n",
      " 337 253 244 132 505 326 247 290 348 122 286  25 571 428 367 389 136 399\n",
      " 270 173 491 327  22  46 446 194 473 526 545 172 465 272 587 409 120 364\n",
      " 146 477 438  43 208 464 302 560 444 187 566 113 295  90 118 352 394 420\n",
      " 459  31 350 309 506 135 242 467 332 201 335 469  34 115 363 223  27 423\n",
      " 226   2  73 549 454 559 169 196  35 573 445 258 189 334  48 448 107 217\n",
      " 588 322 149 205 356 468 249 369  66 360  21 590 213 439 108 520 429 160\n",
      " 417 463 401 585 425 296 383 104 262 371  54  71 481 609 595 143 121 154\n",
      "  19 554 521  75 339 365  10 400 299 292   0 109 280 319 398 219 166   1\n",
      " 537 237  52 474  64 434 482  95 100 519 126  49  51 596 250 103]\n",
      "[523 279 366 321 576  86 234 470 211 608]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "We randomly select 'nUsers' users with their ratings. We first fix the seed\n",
    "of the random generator to make sure that we always get the same 'nUsers'\n",
    "\"\"\"\n",
    "np.random.seed(YourStudentID)\n",
    "indexes_users = np.random.permutation(n_users) # 配列をランダムに並び替え\n",
    "print(indexes_users)\n",
    "\n",
    "my_batch_users = indexes_users[0:nUsersInExample]\n",
    "print(my_batch_users) # nUsers...: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "[[523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523.\n",
      "  523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523.\n",
      "  523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523.\n",
      "  523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523.\n",
      "  523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523. 523.\n",
      "  523. 523. 523. 523. 523.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We will use now the list of 'my_batch_users' to create a matrix Y. \n",
    "\"\"\"\n",
    "# We need to make a list of the movies that these users have watched\n",
    "list_movies_each_user = [[] for _ in range(nUsersInExample)]\n",
    "list_ratings_each_user = [[] for _ in range(nUsersInExample)]\n",
    "# Movies\n",
    "list_movies = ratings['movieId'][ratings['userId'] == my_batch_users[0]].values\n",
    "# print(list_movies)\n",
    "# print(ratings['movieId'])\n",
    "# print(ratings['userId'] == my_batch_users[0])\n",
    "list_movies_each_user[0] = list_movies                    \n",
    "# Ratings                      \n",
    "list_ratings = ratings['rating'][ratings['userId'] == my_batch_users[0]].values\n",
    "list_ratings_each_user[0] = list_ratings\n",
    "\n",
    "# Users\n",
    "n_each_user = list_movies.shape[0]\n",
    "print(n_each_user)\n",
    "list_users = my_batch_users[0]*np.ones((1, n_each_user))\n",
    "print(list_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, nUsersInExample):\n",
    "    # Movies\n",
    "    local_list_per_user_movies = ratings['movieId'][ratings['userId'] == my_batch_users[i]].values\n",
    "    list_movies_each_user[i] = local_list_per_user_movies\n",
    "    list_movies = np.append(list_movies,local_list_per_user_movies)\n",
    "    # Ratings                                 \n",
    "    local_list_per_user_ratings = ratings['rating'][ratings['userId'] == my_batch_users[i]].values\n",
    "    list_ratings_each_user[i] = local_list_per_user_ratings\n",
    "    list_ratings = np.append(list_ratings, local_list_per_user_ratings)  \n",
    "    # Users                                   \n",
    "    n_each_user = local_list_per_user_movies.shape[0]                                                                               \n",
    "    local_rep_user =  my_batch_users[i]*np.ones((1, n_each_user))    \n",
    "    list_users = np.append(list_users, local_rep_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     1      2      3 ... 168248 168252 175303]\n"
     ]
    }
   ],
   "source": [
    "# Let us first see how many unique movies have been rated\n",
    "indexes_unique_movies = np.unique(list_movies)\n",
    "print(indexes_unique_movies)\n",
    "n_movies = indexes_unique_movies.shape[0]\n",
    "\n",
    "# As it is expected no all users have rated all movies. We will build a matrix Y \n",
    "# with NaN inputs and fill according to the data for each user \n",
    "temp = np.empty((n_movies,nUsersInExample,))\n",
    "temp[:] = np.nan\n",
    "Y_with_NaNs = pd.DataFrame(temp)\n",
    "for i in range(nUsersInExample):\n",
    " local_movies = list_movies_each_user[i]\n",
    " ixs = np.in1d(indexes_unique_movies, local_movies) # 1番目の引数の配列の各要素が、2番目の引数の配列に含まれるかどうかの真偽値を返す。\n",
    " Y_with_NaNs.loc[ixs, i] = list_ratings_each_user[i]\n",
    "\n",
    "Y_with_NaNs.index = indexes_unique_movies.tolist()\n",
    "Y_with_NaNs.columns = my_batch_users.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Have a look at the matrix `Y_with_NaNs`. The movies data is now in a data frame which contains one column for each user rating the movie. There are some entries that contain 'NaN'. What does the 'NaN' mean in this context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        523  279  366  321  576  86   234  470  211  608\n",
      "1       NaN  3.0  NaN  NaN  NaN  4.0  5.0  4.0  NaN  2.5\n",
      "2       4.5  NaN  NaN  5.0  NaN  NaN  NaN  3.0  NaN  2.0\n",
      "3       NaN  NaN  NaN  3.0  NaN  NaN  NaN  3.0  NaN  2.0\n",
      "5       NaN  NaN  NaN  3.0  NaN  NaN  NaN  3.0  NaN  NaN\n",
      "6       NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN\n",
      "7       NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN\n",
      "10      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  4.0\n",
      "14      NaN  NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN  NaN\n",
      "16      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  4.5\n",
      "19      NaN  NaN  NaN  3.0  NaN  NaN  NaN  3.0  NaN  2.0\n",
      "21      NaN  NaN  NaN  3.0  NaN  NaN  NaN  3.0  NaN  3.5\n",
      "24      NaN  NaN  NaN  4.0  NaN  NaN  5.0  NaN  NaN  2.0\n",
      "26      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN\n",
      "29      NaN  NaN  NaN  NaN  3.5  NaN  NaN  NaN  NaN  NaN\n",
      "31      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0\n",
      "32      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  3.5\n",
      "34      NaN  NaN  NaN  NaN  NaN  NaN  3.0  4.0  NaN  3.5\n",
      "36      NaN  NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN  NaN\n",
      "39      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  3.0\n",
      "41      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN\n",
      "42      NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN  NaN  NaN\n",
      "43      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN\n",
      "44      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  0.5\n",
      "47      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  4.5\n",
      "48      NaN  NaN  NaN  NaN  NaN  NaN  5.0  NaN  NaN  0.5\n",
      "50      NaN  3.5  NaN  NaN  NaN  NaN  NaN  3.0  4.0  4.5\n",
      "60      NaN  NaN  NaN  NaN  NaN  NaN  4.0  NaN  NaN  NaN\n",
      "62      NaN  NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN\n",
      "63      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  0.5\n",
      "65      NaN  NaN  NaN  NaN  NaN  NaN  3.0  NaN  NaN  2.0\n",
      "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "112552  NaN  3.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "112556  NaN  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "115149  NaN  4.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "115569  NaN  4.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "115617  NaN  NaN  3.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "115713  NaN  5.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "116797  5.0  NaN  4.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "119145  NaN  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "120130  5.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "122882  NaN  4.5  2.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "122886  NaN  3.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "122904  5.0  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "122920  NaN  3.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "127052  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "134130  3.5  3.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "139385  NaN  5.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "140174  NaN  3.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "143859  NaN  3.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "148626  NaN  3.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "150548  5.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "152077  NaN  3.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "152081  4.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "158238  NaN  3.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "160438  NaN  2.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "161582  NaN  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "164179  NaN  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "166528  NaN  3.5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "168248  NaN  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "168252  5.0  4.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "175303  NaN  5.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "\n",
      "[1136 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Y_with_NaNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer Question 2\n",
    "\n",
    "NaN: not a number: A user did not watch a specific movie.\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert our data structure into a form that is appropriate for processing. We will convert the `Y_with_NaNs` dataframe into a new dataframe which contains the user, the movie, and the rating using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([4.5, 5. , 4. , 5. , 4.5, 5. , 4.5, 4.5, 4.5, 4. , 3. , 4.5, 5. ,\n",
      "       5. , 4.5, 5. , 5. , 4.5, 5. , 4.5, 5. , 5. , 5. , 4. , 5. , 5. ,\n",
      "       5. , 5. , 4. , 4.5, 5. , 5. , 5. , 5. , 5. , 5. , 4.5, 4.5, 4. ,\n",
      "       5. , 5. , 4.5, 5. , 4.5, 5. , 4.5, 5. , 5. , 4. , 5. , 5. , 5. ,\n",
      "       5. , 5. , 5. , 5. , 3.5, 4.5, 5. , 4. , 4.5, 5. , 5. , 4. , 5. ,\n",
      "       5. , 5. , 5. , 5. , 5. , 4. , 3.5, 5. , 4.5, 5. ]), array([3. , 3.5, 3.5, 4. , 5. , 5. , 4.5, 3. , 3.5, 4. , 5. , 2. , 4. ,\n",
      "       4. , 2. , 4. , 4. , 4. , 4. , 5. , 4. , 5. , 5. , 4. , 4.5, 3.5,\n",
      "       4.5, 3.5, 5. , 4. , 4. , 3.5, 5. , 3.5, 4.5, 3. , 4. , 4. , 3. ,\n",
      "       4. , 4. , 4. , 5. , 3.5, 4. , 4. , 4.5, 3.5, 3.5, 2. , 3.5, 2.5,\n",
      "       3.5, 5. , 3.5, 3. , 5. , 2.5, 3. , 4. , 3. , 4. , 3. , 1. , 3.5,\n",
      "       3. , 5. , 3. , 3. , 4. , 4. , 2. , 5. , 3. , 3.5, 3.5, 3. , 4. ,\n",
      "       4. , 4. , 3. , 4. , 4.5, 4.5, 3.5, 4. , 3. , 3. , 2.5, 5. , 3.5,\n",
      "       4.5, 3. , 4. , 4. , 2.5, 3.5, 4.5, 3. , 2. , 4. , 4. , 4.5, 3. ,\n",
      "       3. , 4. , 4. , 3.5, 2. , 2. , 3. , 4. , 4. , 4. , 3.5, 4. , 3. ,\n",
      "       3.5, 3.5, 3.5, 3. , 3.5, 5. , 3. , 4. , 3. , 5. , 3.5, 3.5, 3. ,\n",
      "       2. , 3. , 3. , 3. , 4. , 3. , 2. , 2. , 3.5, 5. , 3. , 2. , 2.5,\n",
      "       3. , 4.5, 5. , 3. , 3. , 4.5, 3. , 4. , 4.5, 3.5, 4. , 4.5, 4.5,\n",
      "       5. , 4. , 4.5, 3.5, 4. , 3. , 3. , 5. , 3.5, 3. , 3.5, 3. , 3. ,\n",
      "       2. , 4. , 4. , 3.5, 4. , 4. , 5. ]), array([4. , 4. , 4. , 5. , 4. , 4.5, 4.5, 4. , 4.5, 2. , 3.5, 4. , 4. ,\n",
      "       3.5, 4.5, 4. , 4.5, 4. , 3. , 4. , 3.5, 4.5, 4. , 3.5, 4.5, 4. ,\n",
      "       4. , 5. , 3.5, 4.5, 2. ]), array([5., 3., 3., 3., 3., 4., 4., 4., 5., 3., 4., 3., 3., 4., 3., 4., 3.,\n",
      "       3., 4., 3., 4., 3., 3., 4., 4., 5., 3., 3., 3., 4., 3., 5., 4., 4.,\n",
      "       3., 3., 5., 3., 3., 4., 3., 4., 4., 3., 3., 3., 3., 3., 4., 4., 3.,\n",
      "       3., 3., 4., 4., 5.]), array([3.5, 2.5, 1. , 3. , 1.5, 1. , 4. , 2. , 3. , 4. , 4. , 1.5, 1. ,\n",
      "       5. , 5. , 4.5, 2.5, 4. , 4.5, 4.5]), array([4. , 4. , 4.5, 4.5, 4. , 3.5, 4. , 3. , 4. , 4. , 4. , 4.5, 3.5,\n",
      "       3.5, 4. , 4. , 4. , 4. , 4.5, 3.5, 4. , 4. , 3.5, 3.5, 4. , 4.5,\n",
      "       3.5, 3.5, 4. , 4. , 4. , 4. , 3.5, 4. , 3.5, 4. , 4. , 3.5, 4.5,\n",
      "       4. , 3. , 4. , 4. , 4.5, 4. , 3.5, 4.5, 4. , 4. , 4. , 4.5, 4. ,\n",
      "       3.5, 4. , 4. , 4.5, 5. , 4. , 1.5, 2.5, 4.5, 4. , 3. , 4.5, 4.5,\n",
      "       4.5, 4.5, 3.5, 4. , 4.5]), array([5., 5., 3., 4., 5., 4., 3., 2., 5., 5., 2., 2., 2., 2., 2., 5., 5.,\n",
      "       2., 4., 2., 5., 1., 4., 3., 3., 3., 3., 2., 5., 4., 1., 4., 2., 3.,\n",
      "       5., 3., 2., 5., 4., 4., 5., 4., 4., 3., 4., 2., 5., 4., 3., 3., 3.,\n",
      "       4., 4., 5., 4., 3., 4., 4., 4., 4., 5., 3., 5., 4., 4., 5., 5., 2.,\n",
      "       4., 4., 3., 3., 4., 3., 3., 3., 5., 4., 3., 4., 1., 3., 5., 3., 4.,\n",
      "       2., 2., 5., 4., 4., 3., 3., 3., 4., 1., 4., 3., 2., 5., 2., 3., 4.,\n",
      "       3., 4., 3., 5., 5., 2., 4., 5., 5., 3., 4., 4., 4., 3., 3., 3., 4.,\n",
      "       4., 5., 3., 5., 3., 3., 4., 2., 3., 4., 5., 4., 3., 2., 4., 3., 3.,\n",
      "       2., 2., 3., 3., 3., 3., 5., 4., 3., 4., 3., 5., 3., 2., 2., 5., 3.,\n",
      "       5., 5., 5., 1., 4., 5., 3., 5., 4., 4., 5., 3., 2., 4., 3., 3., 4.,\n",
      "       3., 2., 3., 3., 4., 4., 2., 5., 4., 4., 1., 4., 3., 2., 4., 4., 4.,\n",
      "       3., 3., 4., 4., 4., 3., 3., 3., 5., 2., 4., 1., 4., 4., 4.]), array([4., 3., 3., 3., 3., 3., 3., 4., 3., 3., 3., 3., 4., 4., 3., 3., 3.,\n",
      "       3., 3., 3., 3., 5., 3., 4., 3., 3., 3., 3., 3., 3., 3., 3., 4., 3.,\n",
      "       3., 5., 3., 4., 3., 4., 4., 4., 3., 4., 5., 4., 4., 4., 4., 5., 4.,\n",
      "       3., 3., 3., 4., 5., 4., 4., 4., 5., 4., 5., 3., 5., 4., 3., 3., 4.,\n",
      "       3., 3., 3., 3., 3., 3., 4., 3., 3., 3., 3., 4.]), array([4. , 4.5, 4.5, 5. , 4.5, 4.5, 4. , 4.5, 4. , 4. , 4.5, 4.5, 4. ,\n",
      "       3.5, 4. , 3.5, 4.5, 2. , 5. , 4. , 3. , 3.5, 4. , 3.5, 3.5, 4.5,\n",
      "       5. , 3.5, 4. , 3.5, 3.5, 3.5, 5. , 4.5, 3. , 4. , 5. , 4.5, 3. ,\n",
      "       3.5, 4.5, 4.5, 4.5, 2.5, 2. , 4. , 3.5, 1. , 4. , 4. , 4. , 5. ,\n",
      "       4. , 3.5, 3.5, 4. , 3.5, 4. , 4.5, 4. , 4. , 4. , 3.5, 4.5, 4.5,\n",
      "       4. , 5. , 4. , 4. , 4. , 4. , 2.5, 4. , 4. , 2. , 3.5, 4. , 4.5,\n",
      "       4.5, 2. , 2. , 4. , 5. , 3.5, 4. , 4. , 5. , 4. , 4.5]), array([2.5, 2. , 2. , 4. , 4.5, 2. , 3.5, 2. , 3. , 3.5, 3.5, 3. , 0.5,\n",
      "       4.5, 0.5, 4.5, 0.5, 2. , 3. , 2.5, 2.5, 2. , 3. , 3. , 4. , 3. ,\n",
      "       3. , 3.5, 2. , 2.5, 0.5, 2.5, 3. , 0.5, 4.5, 4. , 3. , 1.5, 2.5,\n",
      "       0.5, 3. , 3.5, 3. , 3. , 2.5, 3. , 2.5, 2.5, 3. , 3.5, 3. , 3.5,\n",
      "       3. , 2.5, 3. , 2.5, 4. , 5. , 0.5, 3. , 3. , 4.5, 3.5, 3. , 4. ,\n",
      "       2.5, 3.5, 4. , 2. , 3. , 3. , 3.5, 2.5, 2. , 3. , 2. , 0.5, 2.5,\n",
      "       3. , 3. , 4. , 1.5, 3.5, 2.5, 2.5, 3. , 2.5, 2.5, 1.5, 0.5, 4.5,\n",
      "       2.5, 4. , 1. , 3. , 1.5, 1.5, 1.5, 3. , 2. , 4. , 2.5, 1. , 2.5,\n",
      "       0.5, 4. , 1.5, 3.5, 2.5, 3.5, 3.5, 0.5, 3.5, 1.5, 3.5, 3.5, 0.5,\n",
      "       1. , 2.5, 3. , 3. , 3. , 3. , 4. , 2.5, 2.5, 0.5, 2.5, 3. , 3.5,\n",
      "       3.5, 3.5, 3. , 3. , 4. , 3. , 3. , 2. , 1. , 3. , 4. , 3. , 0.5,\n",
      "       3.5, 1.5, 4. , 2. , 3. , 3. , 2.5, 3. , 2.5, 3. , 2.5, 4. , 3. ,\n",
      "       3.5, 1. , 3.5, 0.5, 3. , 3. , 1. , 3. , 2. , 5. , 3. , 2.5, 3.5,\n",
      "       3. , 0.5, 2.5, 2.5, 1.5, 2.5, 3.5, 3. , 2. , 3. , 3.5, 3. , 4. ,\n",
      "       3. , 3. , 3.5, 2. , 4. , 0.5, 2.5, 2.5, 3. , 3. , 3. , 3. , 3.5,\n",
      "       4. , 4.5, 3.5, 3. , 4. , 4.5, 4. , 3.5, 3. , 4. , 4.5, 3. , 3.5,\n",
      "       3.5, 3. , 4. , 4. , 3.5, 4. , 2. , 2.5, 4. , 2.5, 2. , 2. , 4. ,\n",
      "       3. , 4. , 3.5, 2. , 3.5, 3.5, 2.5, 2.5, 3.5, 3. , 4. , 1.5, 3. ,\n",
      "       2.5, 1.5, 3. , 2. , 4.5, 2.5, 3. , 3. , 3. , 4. , 0.5, 3. , 4. ,\n",
      "       3.5, 3. , 2. , 2. , 4. , 2.5, 3.5, 2.5, 2.5, 4. , 4. , 3. , 3.5,\n",
      "       2.5, 3. , 3.5, 3. , 3.5, 3. , 3. , 4. , 1. , 2.5, 3. , 4.5, 5. ,\n",
      "       3.5, 3.5, 0.5, 4.5, 4. , 2. , 4.5, 1. , 3.5, 2. , 3. , 2.5, 4.5,\n",
      "       4. , 5. , 4. , 4. , 4.5, 0.5, 4. , 2.5, 2.5, 3.5, 3.5, 3. , 4.5,\n",
      "       3. , 3.5, 3.5, 2.5, 1.5, 3.5, 3.5, 3.5, 3.5, 3. , 4.5, 3.5, 4.5,\n",
      "       4.5, 4.5, 4. , 3. , 3. , 4.5, 3.5, 3.5, 3. , 2.5, 2. , 2.5, 3. ,\n",
      "       2.5, 2.5, 3.5, 4.5, 4.5, 1.5, 1. , 1.5, 2. , 3. , 0.5, 1. , 3. ,\n",
      "       3. , 3. , 3. , 2.5, 3. , 4.5, 3.5, 3. , 4. , 2.5, 1. , 3. , 1.5,\n",
      "       2. , 4. , 3. , 4. , 3. , 2. , 3.5, 3.5, 5. , 4. , 1. , 2.5, 3. ,\n",
      "       3. , 3. , 3.5, 4. , 2. , 3. , 3.5, 2.5, 4. , 2.5, 1.5, 4. , 4. ,\n",
      "       2.5, 3.5, 2. , 3. , 2. , 3.5, 4. , 3. , 2. , 1.5, 4.5, 5. , 5. ,\n",
      "       3.5, 3.5, 3. , 4. , 2. , 2.5, 3. , 3.5, 4.5, 5. , 4. , 1. , 4.5,\n",
      "       2. , 3.5, 4. , 4. , 2.5, 3.5, 4. , 3. , 1.5, 3. , 2.5, 2.5, 3. ,\n",
      "       1.5, 3. , 3. , 3. , 3. , 1. , 4.5, 4.5, 4. , 2.5, 1. , 0.5, 3.5,\n",
      "       1. , 2. , 4. , 3.5, 5. , 3. , 3.5, 1. , 4. , 2. , 4. , 1. , 5. ,\n",
      "       2. , 1.5, 3. , 3. , 2. , 3.5, 3.5, 3. , 4. , 4.5, 4. , 4. , 2.5,\n",
      "       2.5, 2. , 4. , 4.5, 4. , 4. , 0.5, 3. , 3. , 2. , 2. , 1.5, 4. ,\n",
      "       3. , 2.5, 2.5, 3. , 2.5, 3. , 2. , 2. , 1. , 4. , 3. , 3.5, 4.5,\n",
      "       4.5, 4.5, 2.5, 2.5, 3.5, 2.5, 4. , 1.5, 3. , 2.5, 3. , 3.5, 2. ,\n",
      "       2. , 0.5, 3.5, 3. , 2. , 2.5, 4. , 4.5, 3. , 3. , 3. , 4.5, 3. ,\n",
      "       2.5, 3. , 4. , 2. , 2. , 3. , 3.5, 2. , 3.5, 1.5, 1. , 2. , 3.5,\n",
      "       3.5, 4. , 2. , 4. , 4. , 3. , 2.5, 4. , 3. , 1. , 2. , 1.5, 3.5,\n",
      "       4. , 2. , 1. , 2. , 4. , 3. , 4. , 3. , 4. , 5. , 2. , 3. , 3.5,\n",
      "       2.5, 2.5, 3.5, 4.5, 4. , 5. , 2.5, 4. , 4. , 3.5, 3. , 2.5, 4.5,\n",
      "       4.5, 1. , 4.5, 3. , 2.5, 3. , 1.5, 3.5, 3. , 4.5, 3.5, 3.5, 1.5,\n",
      "       4.5, 4.5, 3. , 4. , 3. , 1. , 0.5, 4. , 3.5, 4.5, 0.5, 3. , 1. ,\n",
      "       4. , 4.5, 3. , 3.5, 4. , 4.5, 2. , 2. , 2. , 4.5, 2. , 4.5, 3. ,\n",
      "       4. , 2. , 2.5, 2. , 3.5, 0.5, 2.5, 3. , 3. , 3. , 2.5, 3. , 3.5,\n",
      "       0.5, 0.5, 3.5, 4.5, 2.5, 0.5, 1.5, 4.5, 3. , 2.5, 4. , 4. , 3. ,\n",
      "       3. , 3.5, 4.5, 4. , 3.5, 4. , 3. , 4. , 4.5, 3. , 4. , 2.5, 3.5,\n",
      "       4.5, 4.5, 2.5, 3. , 4. , 4. , 4.5, 0.5, 3.5, 4. , 4. , 4.5, 3.5,\n",
      "       4.5, 4.5, 4.5, 4.5, 3. , 1.5, 3. , 3. , 4.5, 4. , 2. , 4.5, 3.5,\n",
      "       4.5, 3. , 1. , 4. , 4. , 4. , 3.5, 4.5, 1. , 3.5, 4.5, 3.5, 3.5,\n",
      "       4.5, 2. , 3. , 3.5, 4.5, 4. , 5. , 2.5, 3. , 4.5, 4. , 4. , 4. ,\n",
      "       4. , 4. , 4.5, 3. , 4. , 4. , 4. , 1. , 4. , 3.5, 4.5, 4. , 4. ,\n",
      "       4. , 2.5, 4. , 2. , 4. , 5. , 4.5, 4. , 4.5, 3.5, 3.5, 2. , 4. ,\n",
      "       4. , 4.5, 4. , 4.5, 3. , 4.5, 3.5, 4. , 4.5, 3.5, 3.5, 2. , 3.5,\n",
      "       0.5, 2.5, 4.5, 2. , 3. , 4. , 4. , 1.5, 4.5, 5. , 4. , 2.5, 4. ,\n",
      "       3. , 0.5, 4. , 4. , 4. , 4. , 4. , 4. , 3.5, 4. , 4. , 4. , 4. ,\n",
      "       4.5, 4. , 4. , 4.5, 5. , 3.5, 4. , 5. , 4.5, 3.5, 1. , 4. , 4.5,\n",
      "       4.5, 4. , 4. , 4. , 4.5, 3. , 3.5, 3.5, 2.5, 3. , 3.5, 4.5, 4. ,\n",
      "       3.5, 3.5, 3.5, 4. , 4. , 4. , 5. , 3.5, 5. , 4.5, 3.5, 4. , 4. ,\n",
      "       3. , 4.5, 4.5, 4. , 4.5, 4. , 3.5, 3. , 4. , 4. , 5. , 4. , 4. ,\n",
      "       4. , 5. , 3.5, 4.5, 4. , 4.5, 4. , 4. , 4. , 4.5, 3. , 4.5, 4. ,\n",
      "       2. , 4. , 4. , 4. , 3.5, 4.5, 5. , 2.5, 4. , 3. , 5. , 4.5])]\n"
     ]
    }
   ],
   "source": [
    "print(list_ratings_each_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4322085889570553\n",
      "[array([     2,    293,    317,    318,    356,    364,    367,    527,\n",
      "          587,    594,    745,   1485,   1704,   1721,   2959,   3147,\n",
      "         3578,   3949,   4014,   4270,   4973,   4993,   4995,   5444,\n",
      "         5618,   5952,   5971,   5989,   6350,   6539,   7143,   7147,\n",
      "         7153,   7361,   8861,   8970,  26776,  26903,  27731,  31658,\n",
      "        31696,  33615,  37729,  45722,  47610,  52722,  55167,  56174,\n",
      "        56367,  56757,  57504,  58559,  62336,  65261,  74458,  76093,\n",
      "        81591,  82169,  91529,  92259,  95167, 101962, 102407, 103141,\n",
      "       103772, 104283, 106782, 116797, 120130, 122904, 127052, 134130,\n",
      "       150548, 152081, 168252]), array([     1,     50,    104,    110,    260,    296,    318,    356,\n",
      "          377,    480,    541,    588,    589,    593,    608,    648,\n",
      "          780,    858,    924,   1036,   1101,   1136,   1196,   1198,\n",
      "         1200,   1208,   1210,   1213,   1214,   1222,   1240,   1247,\n",
      "         1258,   1270,   1291,   1370,   1544,   1608,   1676,   1732,\n",
      "         2028,   2115,   2288,   2329,   2403,   2502,   2571,   2716,\n",
      "         2791,   2797,   2858,   2918,   2959,   3527,   3578,   3753,\n",
      "         4027,   4223,   4226,   4816,   4886,   4993,   5152,   5378,\n",
      "         5418,   5903,   5952,   6664,   6874,   7143,   7153,   7438,\n",
      "         8376,   8665,   8874,   8950,  33493,  33794,  38061,  44191,\n",
      "        47610,  48516,  48774,  48780,  49272,  51255,  51935,  54286,\n",
      "        54503,  55363,  55820,  56782,  57368,  57669,  58559,  59315,\n",
      "        61024,  61132,  63082,  63131,  64614,  68157,  68237,  68319,\n",
      "        69640,  70286,  71535,  71838,  72641,  72998,  73017,  74458,\n",
      "        79132,  79702,  80463,  80489,  82459,  83134,  85414,  88129,\n",
      "        89745,  90405,  91529,  91542,  93838,  94864,  96079,  96737,\n",
      "        97304,  97306,  97913,  97938,  98809,  99112,  99114, 101864,\n",
      "       102903, 103341, 104841, 104879, 105504, 106489, 107069, 107406,\n",
      "       109374, 109487, 110501, 111362, 111759, 112138, 112171, 112183,\n",
      "       112552, 112556, 115149, 115569, 115713, 119145, 122882, 122886,\n",
      "       122904, 122920, 134130, 139385, 140174, 143859, 148626, 152077,\n",
      "       158238, 160438, 161582, 164179, 166528, 168248, 168252, 175303]), array([   110,    589,   1036,   1089,   2028,   2959,   3578,   3949,\n",
      "         4993,   5481,   6537,   6539,   6874,   6942,   7153,   7438,\n",
      "         8368,   8961,  32587,  33794,  44191,  48516,  58559,  59315,\n",
      "        68157,  79132,  91529, 109487, 115617, 116797, 122882]), array([  2,   3,   5,  19,  21,  24, 141, 150, 153, 158, 160, 161, 165,\n",
      "       186, 208, 225, 231, 237, 252, 253, 256, 276, 288, 292, 316, 317,\n",
      "       339, 344, 349, 350, 355, 356, 364, 367, 371, 377, 380, 410, 440,\n",
      "       454, 457, 480, 500, 539, 586, 587, 588, 589, 592, 593, 595, 597,\n",
      "       648, 736, 780, 802]), array([  29,  327,  372,  720,  849, 1188, 1223, 1231, 1261, 1321, 1345,\n",
      "       1347, 1373, 1479, 2289, 2580, 2641, 2723, 2826, 3082]), array([    1,   165,   260,   318,   356,   419,   457,   480,   588,\n",
      "         590,   595,   648,   780,  1091,  1097,  1196,  1210,  1265,\n",
      "        1680,  1721,  1835,  1961,  2028,  2411,  2421,  2571,  2628,\n",
      "        2642,  2687,  2762,  2805,  2858,  2959,  3300,  3555,  3578,\n",
      "        3793,  3996,  4226,  4306,  4310,  4886,  4901,  4963,  4993,\n",
      "        5266,  5418,  5445,  5952,  6373,  6377,  6539,  6947,  7147,\n",
      "        7153,  7361, 33679, 33794, 51662, 58293, 58559, 59315, 60069,\n",
      "       66097, 69951, 71264, 72356, 74553, 79132, 95510]), array([   1,   24,   34,   42,   48,   60,   65,  126,  153,  158,  169,\n",
      "        172,  181,  231,  258,  260,  317,  344,  353,  355,  364,  374,\n",
      "        377,  427,  438,  442,  455,  466,  480,  485,  502,  517,  546,\n",
      "        552,  553,  577,  586,  588,  590,  592,  594,  610,  648,  673,\n",
      "        688,  720,  736,  737,  762,  801,  888,  917,  919, 1011, 1012,\n",
      "       1021, 1028, 1030, 1036, 1073, 1097, 1101, 1196, 1198, 1200, 1210,\n",
      "       1214, 1215, 1259, 1275, 1291, 1356, 1359, 1370, 1373, 1374, 1377,\n",
      "       1378, 1379, 1391, 1405, 1408, 1409, 1460, 1479, 1495, 1562, 1566,\n",
      "       1573, 1580, 1582, 1587, 1688, 1690, 1707, 1784, 1806, 1826, 1907,\n",
      "       1920, 1967, 2000, 2002, 2005, 2028, 2050, 2052, 2053, 2054, 2080,\n",
      "       2081, 2082, 2088, 2089, 2090, 2105, 2115, 2123, 2137, 2140, 2141,\n",
      "       2142, 2161, 2162, 2193, 2231, 2253, 2294, 2354, 2355, 2373, 2376,\n",
      "       2384, 2406, 2409, 2414, 2421, 2422, 2450, 2528, 2566, 2568, 2571,\n",
      "       2613, 2616, 2617, 2628, 2640, 2641, 2642, 2643, 2657, 2700, 2710,\n",
      "       2720, 2761, 2953, 2985, 2987, 3054, 3114, 3175, 3176, 3274, 3285,\n",
      "       3287, 3396, 3397, 3398, 3418, 3438, 3439, 3440, 3479, 3483, 3489,\n",
      "       3564, 3624, 3672, 3699, 3751, 3793, 3799, 3877, 3977, 3990, 4022,\n",
      "       4084, 4085, 4203, 4226, 4232, 4239, 4241, 4270, 4306, 4321, 4351,\n",
      "       4477, 4519, 4571, 4638]), array([   1,    2,    3,    5,    6,    7,   10,   14,   19,   21,   26,\n",
      "         32,   34,   36,   39,   41,   43,   47,   50,   62,   95,  110,\n",
      "        150,  153,  161,  165,  185,  208,  225,  231,  253,  261,  266,\n",
      "        282,  288,  292,  296,  300,  316,  318,  329,  339,  344,  349,\n",
      "        356,  357,  364,  367,  377,  380,  410,  420,  434,  454,  457,\n",
      "        480,  494,  500,  515,  527,  539,  587,  588,  589,  590,  592,\n",
      "        595,  597,  608,  613,  628,  648,  653,  719,  736,  761,  762,\n",
      "        786,  788, 1073]), array([   50,   260,   296,   318,   356,   858,  1025,  1036,  1125,\n",
      "        1136,  1196,  1198,  1210,  1221,  1222,  1223,  1246,  1556,\n",
      "        1704,  1801,  1831,  1835,  2028,  2087,  2105,  2542,  2571,\n",
      "        2642,  2858,  2947,  2948,  2949,  2959,  3034,  3264,  3481,\n",
      "        3578,  3638,  3869,  3988,  4011,  4226,  4262,  4367,  5266,\n",
      "        5418,  5445,  6934,  7153,  8665, 32587, 33166, 33794, 36529,\n",
      "       44665, 48738, 48774, 48780, 49272, 49530, 54286, 55765, 56367,\n",
      "       57669, 58559, 59315, 60684, 63113, 68358, 69122, 69481, 70286,\n",
      "       71535, 73017, 74458, 77561, 78499, 79132, 79702, 82461, 85414,\n",
      "       87232, 89745, 89864, 90405, 91500, 91529, 91542, 95510]), array([    1,     2,     3,    10,    16,    19,    21,    24,    31,\n",
      "          32,    34,    39,    44,    47,    48,    50,    63,    65,\n",
      "          70,    88,    93,    95,   104,   107,   110,   111,   141,\n",
      "         145,   150,   153,   158,   160,   163,   168,   170,   172,\n",
      "         173,   174,   180,   181,   185,   188,   193,   196,   203,\n",
      "         208,   216,   223,   231,   252,   253,   260,   276,   277,\n",
      "         288,   292,   293,   296,   312,   316,   317,   318,   327,\n",
      "         329,   333,   339,   344,   353,   355,   356,   357,   364,\n",
      "         366,   367,   368,   370,   374,   377,   379,   380,   383,\n",
      "         393,   407,   410,   420,   423,   429,   430,   432,   435,\n",
      "         441,   442,   448,   455,   457,   466,   471,   473,   480,\n",
      "         500,   508,   514,   519,   520,   522,   527,   531,   533,\n",
      "         539,   541,   543,   546,   551,   552,   553,   555,   575,\n",
      "         586,   587,   588,   589,   590,   592,   593,   594,   595,\n",
      "         596,   597,   608,   610,   648,   653,   661,   663,   671,\n",
      "         673,   694,   708,   719,   724,   733,   736,   737,   741,\n",
      "         743,   750,   762,   778,   780,   782,   784,   785,   786,\n",
      "         788,   799,   802,   809,   829,   832,   833,   837,   839,\n",
      "         842,   849,   852,   858,   886,   919,   923,   924,   953,\n",
      "        1015,  1020,  1021,  1023,  1025,  1036,  1037,  1042,  1049,\n",
      "        1059,  1060,  1061,  1064,  1073,  1080,  1089,  1091,  1092,\n",
      "        1097,  1101,  1127,  1129,  1136,  1193,  1196,  1200,  1206,\n",
      "        1208,  1210,  1213,  1214,  1215,  1219,  1220,  1221,  1222,\n",
      "        1240,  1249,  1250,  1258,  1261,  1265,  1268,  1270,  1271,\n",
      "        1274,  1275,  1282,  1288,  1291,  1302,  1320,  1339,  1342,\n",
      "        1347,  1350,  1377,  1387,  1391,  1393,  1396,  1405,  1407,\n",
      "        1409,  1431,  1432,  1464,  1466,  1476,  1479,  1485,  1499,\n",
      "        1500,  1513,  1517,  1527,  1544,  1552,  1556,  1562,  1566,\n",
      "        1573,  1580,  1584,  1586,  1589,  1590,  1591,  1597,  1603,\n",
      "        1608,  1609,  1610,  1615,  1617,  1620,  1625,  1639,  1641,\n",
      "        1644,  1645,  1653,  1673,  1676,  1681,  1682,  1690,  1702,\n",
      "        1704,  1707,  1717,  1721,  1722,  1726,  1729,  1732,  1748,\n",
      "        1753,  1754,  1755,  1760,  1769,  1772,  1777,  1779,  1784,\n",
      "        1792,  1799,  1801,  1831,  1833,  1862,  1882,  1907,  1909,\n",
      "        1911,  1917,  1918,  1921,  1923,  1953,  1954,  1961,  1968,\n",
      "        1991,  1994,  1997,  2000,  2001,  2002,  2003,  2004,  2006,\n",
      "        2009,  2011,  2012,  2021,  2023,  2028,  2042,  2052,  2054,\n",
      "        2060,  2080,  2081,  2082,  2083,  2094,  2097,  2105,  2107,\n",
      "        2115,  2117,  2123,  2124,  2126,  2133,  2134,  2140,  2141,\n",
      "        2142,  2153,  2161,  2167,  2174,  2188,  2193,  2194,  2231,\n",
      "        2232,  2247,  2253,  2278,  2279,  2282,  2291,  2294,  2296,\n",
      "        2315,  2321,  2325,  2329,  2335,  2338,  2353,  2355,  2375,\n",
      "        2384,  2392,  2407,  2423,  2428,  2431,  2454,  2468,  2470,\n",
      "        2474,  2490,  2502,  2505,  2529,  2541,  2542,  2548,  2549,\n",
      "        2550,  2560,  2568,  2571,  2580,  2581,  2600,  2606,  2616,\n",
      "        2617,  2628,  2640,  2672,  2676,  2683,  2699,  2700,  2701,\n",
      "        2706,  2707,  2710,  2712,  2716,  2717,  2719,  2723,  2761,\n",
      "        2762,  2763,  2797,  2798,  2799,  2805,  2808,  2827,  2840,\n",
      "        2841,  2858,  2889,  2890,  2908,  2912,  2916,  2918,  2953,\n",
      "        2959,  2985,  2986,  2987,  2995,  2997,  3005,  3032,  3034,\n",
      "        3052,  3053,  3081,  3082,  3087,  3114,  3146,  3147,  3148,\n",
      "        3160,  3175,  3176,  3186,  3210,  3243,  3247,  3248,  3252,\n",
      "        3253,  3255,  3257,  3258,  3264,  3267,  3268,  3269,  3270,\n",
      "        3271,  3273,  3274,  3275,  3298,  3300,  3316,  3354,  3355,\n",
      "        3361,  3362,  3388,  3396,  3408,  3409,  3421,  3438,  3439,\n",
      "        3440,  3448,  3450,  3461,  3471,  3477,  3481,  3484,  3489,\n",
      "        3499,  3510,  3527,  3534,  3552,  3578,  3593,  3608,  3617,\n",
      "        3623,  3646,  3686,  3688,  3690,  3698,  3717,  3735,  3745,\n",
      "        3751,  3752,  3753,  3763,  3785,  3793,  3798,  3799,  3802,\n",
      "        3809,  3825,  3863,  3864,  3868,  3869,  3897,  3911,  3915,\n",
      "        3917,  3948,  3949,  3953,  3968,  3977,  3979,  3986,  3988,\n",
      "        3994,  3996,  4011,  4015,  4016,  4018,  4019,  4022,  4025,\n",
      "        4027,  4034,  4040,  4052,  4054,  4085,  4090,  4104,  4105,\n",
      "        4128,  4148,  4161,  4178,  4214,  4223,  4226,  4238,  4239,\n",
      "        4246,  4247,  4255,  4262,  4270,  4306,  4308,  4310,  4321,\n",
      "        4343,  4344,  4366,  4367,  4369,  4370,  4386,  4388,  4396,\n",
      "        4446,  4447,  4448,  4452,  4487,  4544,  4545,  4558,  4571,\n",
      "        4621,  4623,  4636,  4638,  4641,  4642,  4643,  4677,  4678,\n",
      "        4679,  4719,  4720,  4725,  4733,  4734,  4776,  4816,  4821,\n",
      "        4865,  4866,  4873,  4874,  4876,  4878,  4886,  4887,  4890,\n",
      "        4896,  4901,  4963,  4974,  4975,  4980,  4992,  4993,  4995,\n",
      "        5025,  5049,  5064,  5065,  5072,  5103,  5146,  5171,  5218,\n",
      "        5219,  5220,  5254,  5266,  5293,  5294,  5299,  5308,  5313,\n",
      "        5349,  5378,  5388,  5401,  5418,  5444,  5445,  5449,  5452,\n",
      "        5459,  5464,  5502,  5507,  5528,  5597,  5618,  5630,  5668,\n",
      "        5669,  5679,  5785,  5816,  5872,  5901,  5902,  5903,  5944,\n",
      "        5945,  5952,  5954,  5956,  5959,  5989,  5991,  6003,  6016,\n",
      "        6059,  6104,  6157,  6166,  6187,  6188,  6193,  6281,  6287,\n",
      "        6294,  6296,  6323,  6331,  6333,  6365,  6373,  6377,  6378,\n",
      "        6502,  6503,  6534,  6537,  6539,  6541,  6595,  6618,  6659,\n",
      "        6708,  6709,  6710,  6754,  6764,  6796,  6807,  6820,  6827,\n",
      "        6857,  6863,  6868,  6870,  6874,  6879,  6888,  6893,  6934,\n",
      "        6936,  6953,  6957,  6979,  7004,  7076,  7090,  7137,  7143,\n",
      "        7147,  7153,  7158,  7163,  7254,  7324,  7325,  7346,  7347,\n",
      "        7360,  7361,  7373,  7377,  7438,  7439,  7445,  7451,  7701,\n",
      "        7773,  8132,  8360,  8361,  8366,  8368,  8371,  8376,  8464,\n",
      "        8528,  8529,  8622,  8636,  8644,  8665,  8667,  8781,  8783,\n",
      "        8784,  8798,  8807,  8810,  8873,  8874,  8939,  8957,  8961,\n",
      "        8966,  8967,  8970,  8972,  8983,  8984, 27706, 27839, 30810,\n",
      "       30822, 31221, 31696, 32029, 32031, 32302, 32587, 33437, 33679,\n",
      "       33725, 34048, 34319, 34405, 36708, 37386, 37830, 40278, 42738,\n",
      "       43928, 44191, 44665, 45499, 49284, 51255, 51662, 51903, 51935,\n",
      "       52245, 53996, 54503])]\n",
      "[523. 523. 523. ... 608. 608. 608.]\n",
      "      movies   ratings  ratingsorig  users\n",
      "0          2  1.067791          4.5  523.0\n",
      "1        293  1.567791          5.0  523.0\n",
      "2        317  0.567791          4.0  523.0\n",
      "3        318  1.567791          5.0  523.0\n",
      "4        356  1.067791          4.5  523.0\n",
      "5        364  1.567791          5.0  523.0\n",
      "6        367  1.067791          4.5  523.0\n",
      "7        527  1.067791          4.5  523.0\n",
      "8        587  1.067791          4.5  523.0\n",
      "9        594  0.567791          4.0  523.0\n",
      "10       745 -0.432209          3.0  523.0\n",
      "11      1485  1.067791          4.5  523.0\n",
      "12      1704  1.567791          5.0  523.0\n",
      "13      1721  1.567791          5.0  523.0\n",
      "14      2959  1.067791          4.5  523.0\n",
      "15      3147  1.567791          5.0  523.0\n",
      "16      3578  1.567791          5.0  523.0\n",
      "17      3949  1.067791          4.5  523.0\n",
      "18      4014  1.567791          5.0  523.0\n",
      "19      4270  1.067791          4.5  523.0\n",
      "20      4973  1.567791          5.0  523.0\n",
      "21      4993  1.567791          5.0  523.0\n",
      "22      4995  1.567791          5.0  523.0\n",
      "23      5444  0.567791          4.0  523.0\n",
      "24      5618  1.567791          5.0  523.0\n",
      "25      5952  1.567791          5.0  523.0\n",
      "26      5971  1.567791          5.0  523.0\n",
      "27      5989  1.567791          5.0  523.0\n",
      "28      6350  0.567791          4.0  523.0\n",
      "29      6539  1.067791          4.5  523.0\n",
      "...      ...       ...          ...    ...\n",
      "1600   30822  0.567791          4.0  608.0\n",
      "1601   31221  0.567791          4.0  608.0\n",
      "1602   31696  1.567791          5.0  608.0\n",
      "1603   32029  0.567791          4.0  608.0\n",
      "1604   32031  0.567791          4.0  608.0\n",
      "1605   32302  0.567791          4.0  608.0\n",
      "1606   32587  1.567791          5.0  608.0\n",
      "1607   33437  0.067791          3.5  608.0\n",
      "1608   33679  1.067791          4.5  608.0\n",
      "1609   33725  0.567791          4.0  608.0\n",
      "1610   34048  1.067791          4.5  608.0\n",
      "1611   34319  0.567791          4.0  608.0\n",
      "1612   34405  0.567791          4.0  608.0\n",
      "1613   36708  0.567791          4.0  608.0\n",
      "1614   37386  1.067791          4.5  608.0\n",
      "1615   37830 -0.432209          3.0  608.0\n",
      "1616   40278  1.067791          4.5  608.0\n",
      "1617   42738  0.567791          4.0  608.0\n",
      "1618   43928 -1.432209          2.0  608.0\n",
      "1619   44191  0.567791          4.0  608.0\n",
      "1620   44665  0.567791          4.0  608.0\n",
      "1621   45499  0.567791          4.0  608.0\n",
      "1622   49284  0.067791          3.5  608.0\n",
      "1623   51255  1.067791          4.5  608.0\n",
      "1624   51662  1.567791          5.0  608.0\n",
      "1625   51903 -0.932209          2.5  608.0\n",
      "1626   51935  0.567791          4.0  608.0\n",
      "1627   52245 -0.432209          3.0  608.0\n",
      "1628   53996  1.567791          5.0  608.0\n",
      "1629   54503  1.067791          4.5  608.0\n",
      "\n",
      "[1630 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "p_list_ratings = np.concatenate(list_ratings_each_user).ravel() # ravel関数は、多次元のリストを1次元のリストにして返します。\n",
    "p_list_ratings_original = p_list_ratings.tolist()\n",
    "mean_ratings_train = np.mean(p_list_ratings)\n",
    "print(mean_ratings_train)\n",
    "p_list_ratings =  p_list_ratings - mean_ratings_train # remove the mean\n",
    "\n",
    "p_list_movies = np.concatenate(list_movies_each_user).ravel().tolist()\n",
    "print(list_movies_each_user)\n",
    "\n",
    "p_list_users = list_users.tolist()\n",
    "print(list_users)\n",
    "\n",
    "Y = pd.DataFrame({'users': p_list_users, 'movies': p_list_movies, 'ratingsorig': p_list_ratings_original,'ratings':p_list_ratings.tolist()})\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "The dataframes `Y_with_NaNs` and `Y` contain the same information but organised in a different way. Explain what is the difference. We have also included two columns for ratings in dataframe `Y`, `ratingsorig` and `ratings`. Explain\n",
    "the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Answer\n",
    "\n",
    "Y_with_NaNs have some missing values in the dataframe, while Y does not have those values. It is easier to refer to the specific movieid in Y_with_NaNs, but we have to care about missing values.\n",
    "\n",
    "Because it is not easy to compare ratings compare movies in general, subtracting the value of mean from each value enables us to know what to extent each movie was good or not. Less computational cost.\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "We now need a measure for determining the similarity between the item and the user: how close the user is sitting to the item in the room if you like. We are going to use the inner product between the vector representing the item and the vector representing the user. \n",
    "\n",
    "An inner product (or [dot product](http://en.wikipedia.org/wiki/Dot_product)) between two vectors $\\mathbf{a}$ and $\\mathbf{b}$ is written as $\\mathbf{a}\\cdot\\mathbf{b}$. Or in vector notation we sometimes write it as $\\mathbf{a}^\\top\\mathbf{b}$. An inner product is simply the sum of the products of each element of the vector,\n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{b} = \\sum_{i} a_i b_i\n",
    "$$\n",
    "The inner product can be seen as a measure of similarity. The inner product gives us the cosine of the angle between the two vectors multiplied by their length. The smaller the angle between two vectors the larger the inner product. \n",
    "$$\n",
    "\\mathbf{a}^\\top\\mathbf{b} = |\\mathbf{a}||\\mathbf{b}| \\cos(\\theta)\n",
    "$$\n",
    "where $\\theta$ is the angle between two vectors and $|\\mathbf{a}|$ and $|\\mathbf{b}|$ are the respective lengths of the two vectors.\n",
    "\n",
    "Since we want each user to be sitting near each item, then we want the inner product to be large for any two items which are rated highly by that user. We can do this by trying to force the inner product $\\mathbf{u}_i^\\top\\mathbf{v}_j$ to be similar to the rating given by the user, $y_{i,j}$. To ensure this we will use a least squares objective function for all user ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The error function (or objective function, or cost function) we will choose is known as 'sum of squares', we will aim to minimize the sum of squared squared error between the inner product of $\\mathbf{u}_i$ and $\\mathbf{v}_i$ and the observed score for the user/item pairing, given by $y_{i, j}$. \n",
    "\n",
    "The total objective function can be written as\n",
    "$$\n",
    "E(\\mathbf{U}, \\mathbf{V}) = \\sum_{i,j} s_{i,j} (y_{i,j} - \\mathbf{u}_i^\\top \\mathbf{v}_j)^2\n",
    "$$\n",
    "where $s_{i,j}$ is an *indicator* variable that is 1 if user $i$ has rated item $j$ and is zero otherwise. Here $\\mathbf{U}$ is the matrix made up of all the vectors $\\mathbf{u}$,\n",
    "$$\n",
    "\\mathbf{U} = \\begin{bmatrix} \\mathbf{u}_1 \\dots \\mathbf{u}_n\\end{bmatrix}^\\top\n",
    "$$\n",
    "where we note that $i$th *row* of $\\mathbf{U}$ contains the vector associated with the $i$th user and $n$ is the total number of users. This form of matrix is known as a *design matrix*. Similarly, we define the matrix\n",
    "$$\n",
    "\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}_1 \\dots \\mathbf{v}_m\\end{bmatrix}^\\top\n",
    "$$\n",
    "where again the $j$th row of $\\mathbf{V}$ contains the vector associated with the $j$th item and $m$ is the total number of items in the data set.\n",
    "\n",
    "## Objective Optimization\n",
    "\n",
    "The idea is to mimimize this objective. A standard, simple, technique for minimizing an objective is *gradient descent* or *steepest descent*. In gradient descent we simply choose to update each parameter in the model by subtracting a multiple of the objective function's gradient with respect to the parameters. So for a parameter $u_{i,j}$ from the matrix $\\mathbf{U}$ we would have an update as follows:\n",
    "$$\n",
    "u_{k,\\ell} \\leftarrow u_{k,\\ell} - \\eta \\frac{\\text{d} E(\\mathbf{U}, \\mathbf{V})}{\\text{d}u_{k,\\ell}} \n",
    "$$\n",
    "where $\\eta$ (which is pronounced *eta* in English) is a Greek letter representing the *learning rate*.  \n",
    "\n",
    "We can compute the gradient of the objective function with respect to $u_{k,\\ell}$ as\n",
    "$$\n",
    "\\frac{\\text{d}E(\\mathbf{U}, \\mathbf{V})}{\\text{d}u_{k,\\ell}} = -2 \\sum_j s_{k,j}v_{j,\\ell}(y_{k, j} - \\mathbf{u}_k^\\top\\mathbf{v}_{j}). \n",
    "$$\n",
    "Similarly each parameter $v_{i,j}$ needs to be updated according to its gradient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What is the gradient of the objective function with respect to $v_{k, \\ell}$? Write your answer in the box below, and explain which differentiation techniques you used to get there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Answer\n",
    "\n",
    "$$\n",
    "\\frac{\\text{d}E(\\mathbf{U}, \\mathbf{V})}{\\text{d}v_{k,\\ell}} = -2 \\sum_j s_{k,j}u_{j,\\ell}(y_{k, j} - \\mathbf{u}_k^\\top\\mathbf{v}_{j}). \n",
    "$$\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4 Code Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent Algorithm\n",
    "\n",
    "In the steepest descent algorithm we aim to minimize the objective function by subtacting the gradient of the objective function from the parameters. \n",
    "\n",
    "### Initialisation\n",
    "\n",
    "To start with though, we need initial values for the matrix $\\mathbf{U}$ and the matrix $\\mathbf{V}$. Let's create them as `pandas` data frames and initialise them randomly with small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 2 # the dimension of our map of the 'library'\n",
    "learn_rate = 0.01\n",
    "U = pd.DataFrame(np.random.normal(size=(nUsersInExample, q))*0.001, index=my_batch_users)\n",
    "V = pd.DataFrame(np.random.normal(size=(n_movies, q))*0.001, index=indexes_unique_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the initial values set, we can start the optimization. First we define a function for the gradient of the objective and the objective function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_gradient(Y, U, V):\n",
    "    gU = pd.DataFrame(np.zeros((U.shape)), index=U.index)\n",
    "    gV = pd.DataFrame(np.zeros((V.shape)), index=V.index)\n",
    "    obj = 0.\n",
    "    nrows = Y.shape[0]\n",
    "    for i in range(nrows):\n",
    "        row = Y.iloc[i]\n",
    "        user = row['users']\n",
    "        film = row['movies']\n",
    "        rating = row['ratings']\n",
    "        prediction = np.dot(U.loc[user], V.loc[film]) # vTu\n",
    "        diff = prediction - rating # vTu - y\n",
    "        obj += diff*diff\n",
    "        gU.loc[user] += 2*diff*V.loc[film]\n",
    "        gV.loc[film] += 2*diff*U.loc[user]\n",
    "    return obj, gU, gV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write our simple optimisation route. This allows us to observe the objective function as the optimization proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Objective function:  1769.7591042362885\n",
      "Iteration 1 Objective function:  1769.758929802282\n",
      "Iteration 2 Objective function:  1769.7587192087253\n",
      "Iteration 3 Objective function:  1769.7583868146978\n",
      "Iteration 4 Objective function:  1769.757769728024\n",
      "Iteration 5 Objective function:  1769.7564906516027\n",
      "Iteration 6 Objective function:  1769.7536157194688\n",
      "Iteration 7 Objective function:  1769.746752307249\n",
      "Iteration 8 Objective function:  1769.7296415145634\n",
      "Iteration 9 Objective function:  1769.6856966331702\n",
      "Iteration 10 Objective function:  1769.570604264062\n",
      "Iteration 11 Objective function:  1769.2653955316102\n",
      "Iteration 12 Objective function:  1768.449845850054\n",
      "Iteration 13 Objective function:  1766.2617079723261\n",
      "Iteration 14 Objective function:  1760.3866016677614\n",
      "Iteration 15 Objective function:  1744.681584866552\n",
      "Iteration 16 Objective function:  1703.3617229162796\n",
      "Iteration 17 Objective function:  1599.467076646042\n",
      "Iteration 18 Objective function:  1368.5209806744283\n",
      "Iteration 19 Objective function:  998.5296643326997\n"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "for i in range(iterations):\n",
    "    obj, gU, gV = objective_gradient(Y, U, V)\n",
    "    print(\"Iteration\", i, \"Objective function: \", obj)\n",
    "    U -= learn_rate*gU\n",
    "    V -= learn_rate*gV    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What happens as you increase the number of iterations? What happens if you increase the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 Answer\n",
    "\n",
    "Decrease the values of objective function.\n",
    "\n",
    "Training might be more faster, but it becomes more unstable and sometimes fails.\n",
    "\n",
    "Write your answer to the question in this box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Objective function:  1769.759098340678\n",
      "Iteration 1 Objective function:  1769.7589140198622\n",
      "Iteration 2 Objective function:  1769.7586903791066\n",
      "Iteration 3 Objective function:  1769.7582828542927\n",
      "Iteration 4 Objective function:  1769.7574012250752\n",
      "Iteration 5 Objective function:  1769.7553120408913\n",
      "Iteration 6 Objective function:  1769.7500917379157\n",
      "Iteration 7 Objective function:  1769.7366254472738\n",
      "Iteration 8 Objective function:  1769.701212147795\n",
      "Iteration 9 Objective function:  1769.6069953171968\n",
      "Iteration 10 Objective function:  1769.354589773552\n",
      "Iteration 11 Objective function:  1768.6757110261085\n",
      "Iteration 12 Objective function:  1766.8464276893635\n",
      "Iteration 13 Objective function:  1761.9193404968476\n",
      "Iteration 14 Objective function:  1748.7062554817396\n",
      "Iteration 15 Objective function:  1713.7570438668292\n",
      "Iteration 16 Objective function:  1624.7964099423898\n",
      "Iteration 17 Objective function:  1420.71705310214\n",
      "Iteration 18 Objective function:  1065.8304567660464\n",
      "Iteration 19 Objective function:  764.7172824589275\n",
      "Iteration 20 Objective function:  722.2445627282211\n",
      "Iteration 21 Objective function:  718.1849617169706\n",
      "Iteration 22 Objective function:  713.2274084877891\n",
      "Iteration 23 Objective function:  706.3976958127322\n",
      "Iteration 24 Objective function:  696.7508233529061\n",
      "Iteration 25 Objective function:  683.0040974805215\n",
      "Iteration 26 Objective function:  663.4353040257305\n",
      "Iteration 27 Objective function:  635.9377785937527\n",
      "Iteration 28 Objective function:  598.4388282171018\n",
      "Iteration 29 Objective function:  550.0193851329631\n"
     ]
    }
   ],
   "source": [
    "q = 2 # the dimension of our map of the 'library'\n",
    "learn_rate = 0.01\n",
    "U = pd.DataFrame(np.random.normal(size=(nUsersInExample, q))*0.001, index=my_batch_users)\n",
    "V = pd.DataFrame(np.random.normal(size=(n_movies, q))*0.001, index=indexes_unique_movies)\n",
    "\n",
    "iterations = 30\n",
    "for i in range(iterations):\n",
    "    obj, gU, gV = objective_gradient(Y, U, V)\n",
    "    print(\"Iteration\", i, \"Objective function: \", obj)\n",
    "    U -= learn_rate*gU\n",
    "    V -= learn_rate*gV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Predictions can be made from the model of the appropriate rating for a given user, $i$, for a given film, $j$, by simply taking the inner product between their vectors $\\mathbf{u}_i$ and $\\mathbf{v}_j$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Create a function that provides the prediction of the ratings for the users in the dataset. \n",
    "\n",
    "Is the quality of the predictions affected by the number of iterations or the learning rate? \n",
    "\n",
    "The function should receive `Y`, `U` and `V` and return the predictions and the absolute error between the predictions and the actual rating given by the users. \n",
    "\n",
    "The predictions and the absolute error should be added as additional columns to the dataframe `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      movies   ratings  ratingsorig  users  Prediction  Abs error\n",
      "0          2  1.067791          4.5  523.0   -0.337116   1.404908\n",
      "1        293  1.567791          5.0  523.0    0.264996   1.302796\n",
      "2        317  0.567791          4.0  523.0   -0.051159   0.618951\n",
      "3        318  1.567791          5.0  523.0    0.526814   1.040977\n",
      "4        356  1.067791          4.5  523.0   -0.012374   1.080165\n",
      "5        364  1.567791          5.0  523.0    0.156464   1.411327\n",
      "6        367  1.067791          4.5  523.0   -0.336680   1.404471\n",
      "7        527  1.067791          4.5  523.0    0.222534   0.845258\n",
      "8        587  1.067791          4.5  523.0   -0.195886   1.263678\n",
      "9        594  0.567791          4.0  523.0   -0.179620   0.747411\n",
      "10       745 -0.432209          3.0  523.0   -0.100180   0.332029\n",
      "11      1485  1.067791          4.5  523.0   -0.049552   1.117344\n",
      "12      1704  1.567791          5.0  523.0    0.470498   1.097294\n",
      "13      1721  1.567791          5.0  523.0   -0.290765   1.858556\n",
      "14      2959  1.067791          4.5  523.0    0.577775   0.490016\n",
      "15      3147  1.567791          5.0  523.0    0.264961   1.302830\n",
      "16      3578  1.567791          5.0  523.0    0.363134   1.204658\n",
      "17      3949  1.067791          4.5  523.0    0.521384   0.546407\n",
      "18      4014  1.567791          5.0  523.0    0.364280   1.203511\n",
      "19      4270  1.067791          4.5  523.0    0.079063   0.988729\n",
      "20      4973  1.567791          5.0  523.0    0.363223   1.204568\n",
      "21      4993  1.567791          5.0  523.0    0.452669   1.115122\n",
      "22      4995  1.567791          5.0  523.0    0.405353   1.162438\n",
      "23      5444  0.567791          4.0  523.0    0.057196   0.510595\n",
      "24      5618  1.567791          5.0  523.0    0.124451   1.443341\n",
      "25      5952  1.567791          5.0  523.0    0.474835   1.092957\n",
      "26      5971  1.567791          5.0  523.0    0.361967   1.205824\n",
      "27      5989  1.567791          5.0  523.0    0.264978   1.302814\n",
      "28      6350  0.567791          4.0  523.0    0.133457   0.434334\n",
      "29      6539  1.067791          4.5  523.0    0.256825   0.810966\n",
      "...      ...       ...          ...    ...         ...        ...\n",
      "1600   30822  0.567791          4.0  608.0    0.563954   0.003837\n",
      "1601   31221  0.567791          4.0  608.0    0.564122   0.003669\n",
      "1602   31696  1.567791          5.0  608.0    1.823347   0.255556\n",
      "1603   32029  0.567791          4.0  608.0    0.563902   0.003889\n",
      "1604   32031  0.567791          4.0  608.0    0.563960   0.003832\n",
      "1605   32302  0.567791          4.0  608.0    0.563944   0.003848\n",
      "1606   32587  1.567791          5.0  608.0    1.565970   0.001821\n",
      "1607   33437  0.067791          3.5  608.0    0.067376   0.000416\n",
      "1608   33679  1.067791          4.5  608.0    1.218767   0.150975\n",
      "1609   33725  0.567791          4.0  608.0    0.563938   0.003853\n",
      "1610   34048  1.067791          4.5  608.0    1.060437   0.007354\n",
      "1611   34319  0.567791          4.0  608.0    0.564061   0.003730\n",
      "1612   34405  0.567791          4.0  608.0    0.564006   0.003785\n",
      "1613   36708  0.567791          4.0  608.0    0.564026   0.003766\n",
      "1614   37386  1.067791          4.5  608.0    1.060486   0.007305\n",
      "1615   37830 -0.432209          3.0  608.0   -0.429352   0.002856\n",
      "1616   40278  1.067791          4.5  608.0    1.060581   0.007210\n",
      "1617   42738  0.567791          4.0  608.0    0.563962   0.003830\n",
      "1618   43928 -1.432209          2.0  608.0   -1.422590   0.009619\n",
      "1619   44191  0.567791          4.0  608.0    0.637369   0.069577\n",
      "1620   44665  0.567791          4.0  608.0    0.557381   0.010410\n",
      "1621   45499  0.567791          4.0  608.0    0.563933   0.003858\n",
      "1622   49284  0.067791          3.5  608.0    0.067377   0.000415\n",
      "1623   51255  1.067791          4.5  608.0    1.120443   0.052652\n",
      "1624   51662  1.567791          5.0  608.0    1.326418   0.241373\n",
      "1625   51903 -0.932209          2.5  608.0   -0.925859   0.006349\n",
      "1626   51935  0.567791          4.0  608.0    0.484727   0.083064\n",
      "1627   52245 -0.432209          3.0  608.0   -0.429361   0.002847\n",
      "1628   53996  1.567791          5.0  608.0    1.557270   0.010522\n",
      "1629   54503  1.067791          4.5  608.0    0.893631   0.174160\n",
      "\n",
      "[1630 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Question 6 Code Answer\n",
    "def predict(Y, U, V):\n",
    "    nrows = Y.shape[0]\n",
    "    predictions = []\n",
    "    abs_error = []\n",
    "    true_ratings = []\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        row = Y.iloc[i]\n",
    "        user = row['users']\n",
    "        film = row['movies']\n",
    "        rating = row['ratings']\n",
    "        prediction = np.dot(U.loc[user], V.loc[film])\n",
    "        predictions.append(prediction) # vTu\n",
    "        abs_error.append(abs(prediction - rating))\n",
    "        true_ratings.append(rating)\n",
    "    \n",
    "    Y['Prediction'] = predictions\n",
    "    Y['Abs error'] = abs_error\n",
    "    \n",
    "    return predictions, abs_error, true_ratings\n",
    "\n",
    "predictions, abs_error, true_ratings = predict(Y, U, V)\n",
    "print(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent or Robbins Monroe Algorithm\n",
    "\n",
    "Stochastic gradient descent involves updating separating each gradient update according to each separate observation, rather than summing over them all. It is an approximate optimization method, but it has proven convergence under certain conditions and can be much faster in practice. It is used widely by internet companies for doing machine learning in practice. For example, Facebook's ad ranking algorithm uses stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Create a stochastic gradient descent version of the algorithm. Monitor the objective function after every 1000 updates to ensure that it is decreasing. When you have finished, plot the movie map and the user map in two dimensions (you can use the columns of the matrices $\\mathbf{U}$ for the user map and the columns of $\\mathbf{V}$ for the movie map). Provide three observations about these maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Objective function:  1.1401787217078863\n",
      "Iteration 1000 Objective function:  3.7334383996906584\n",
      "Iteration 2000 Objective function:  0.00459518907243189\n",
      "Iteration 3000 Objective function:  0.18680350885185812\n",
      "Iteration 4000 Objective function:  0.3223842604895278\n",
      "Iteration 5000 Objective function:  2.051232871865473\n",
      "Iteration 6000 Objective function:  5.915485100587443\n",
      "Iteration 7000 Objective function:  2.0511130914464837\n",
      "Iteration 8000 Objective function:  0.18680159092742177\n",
      "Iteration 9000 Objective function:  0.32236990705085483\n",
      "Iteration 10000 Objective function:  0.004595326583852287\n",
      "Iteration 11000 Objective function:  8.587928340661172\n",
      "Iteration 12000 Objective function:  0.004600204177740294\n",
      "Iteration 13000 Objective function:  0.00457422584848321\n",
      "Iteration 14000 Objective function:  0.004532305649647333\n",
      "Iteration 15000 Objective function:  0.004602945924011452\n",
      "Iteration 16000 Objective function:  0.3223466990957207\n",
      "Iteration 17000 Objective function:  0.0045954540958543745\n",
      "Iteration 18000 Objective function:  0.3224026051346668\n",
      "Iteration 19000 Objective function:  0.27154792545590256\n",
      "Iteration 20000 Objective function:  0.14780408004058615\n",
      "Iteration 21000 Objective function:  0.0005308026008557324\n",
      "Iteration 22000 Objective function:  0.14433684028642227\n",
      "Iteration 23000 Objective function:  0.3193812080972055\n",
      "Iteration 24000 Objective function:  0.0015341169390905034\n",
      "Iteration 25000 Objective function:  0.18085138678355886\n",
      "Iteration 26000 Objective function:  0.0003496870684903589\n",
      "Iteration 27000 Objective function:  0.1850572044153203\n",
      "Iteration 28000 Objective function:  4.785591896740992e-05\n",
      "Iteration 29000 Objective function:  1.002957340172691\n",
      "Iteration 30000 Objective function:  2.471652289796573\n",
      "Iteration 31000 Objective function:  0.00038368774548950006\n",
      "Iteration 32000 Objective function:  1.833928614330436e-06\n",
      "Iteration 33000 Objective function:  1.1030309219423102\n",
      "Iteration 34000 Objective function:  0.0003338974765843032\n",
      "Iteration 35000 Objective function:  3.2355400478630536e-06\n",
      "Iteration 36000 Objective function:  4.880090496915228e-06\n",
      "Iteration 37000 Objective function:  1.644085058462494\n",
      "Iteration 38000 Objective function:  2.8909267819602807e-07\n",
      "Iteration 39000 Objective function:  0.1867578630062137\n",
      "Iteration 40000 Objective function:  1.72740894425107\n"
     ]
    }
   ],
   "source": [
    "# Question 7 Code Answer\n",
    "\n",
    "import random\n",
    "\n",
    "def objective_stochastic_gradient(Y, U, V):\n",
    "    gU = pd.DataFrame(np.zeros((U.shape)), index=U.index)\n",
    "    gV = pd.DataFrame(np.zeros((V.shape)), index=V.index)\n",
    "    obj = 0.\n",
    "    nrows = Y.shape[0]\n",
    "    row = Y.iloc[random.randint(0, nrows - 1)]\n",
    "    user = row['users']\n",
    "    film = row['movies']\n",
    "    rating = row['ratings']\n",
    "    prediction = np.dot(U.loc[user], V.loc[film]) # vTu\n",
    "    diff = prediction - rating # vTu - y\n",
    "    obj += diff * diff\n",
    "    gU.loc[user] += 2*diff*V.loc[film]\n",
    "    gV.loc[film] += 2*diff*U.loc[user]\n",
    "        \n",
    "    return obj, gU, gV\n",
    "\n",
    "\n",
    "q = 2 # the dimension of our map of the 'library'\n",
    "learn_rate = 0.01\n",
    "Y = pd.DataFrame({'users': p_list_users, 'movies': p_list_movies, 'ratingsorig': p_list_ratings_original,'ratings':p_list_ratings.tolist()})\n",
    "U = pd.DataFrame(np.random.normal(size=(nUsersInExample, q))*0.001, index=my_batch_users)\n",
    "V = pd.DataFrame(np.random.normal(size=(n_movies, q))*0.001, index=indexes_unique_movies)\n",
    "\n",
    "iterations = 100000\n",
    "for i in range(iterations):\n",
    "    obj, gU, gV = objective_stochastic_gradient(Y, U, V)\n",
    "    U -= learn_rate * gU\n",
    "    V -= learn_rate * gV \n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"Iteration\", i, \"Objective function: \", obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Our Map Enough? Are Our Data Enough?\n",
    "\n",
    "Is two dimensions really enough to capture the complexity of humans and their artforms? Perhaps we need even more dimensions to capture that complexity. Extending our books analogy further, consider how we should place books that have a historical timeframe as well as some geographical location. Do we really want books from the 2nd World War to sit alongside books from the Roman Empire? Books on the American invasion of Sicily in 1943 are perhaps less related to books about Carthage than those that study the Jewish Revolt from 66-70 (in the Roman Province of Judaea). So books that relate to subjects which are closer in time should be stored together. However, a student of rebellion against empire may also be interested in the relationship between the Jewish Revolt of 66-70 and the Indian Rebellion of 1857, nearly 1800 years later. Whilst the technologies are different, the psychology of the people is shared: a rebellious nation angainst their imperial masters, triggered by misrule with a religious and cultural background. To capture such complexities we would need further dimensions in our latent representation. But are further dimensions justified by the amount of data we have? Can we really understand the facets of a film that only has at most three or four ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "\n",
    "If you want to take this model further then you'll need more data. You can use again the MovieLens 100k data but increasing the number of users (for example, for the Steepest Descent Algorithm you can do this by modifying the variable `nUsersInExample` that was set as 10 before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Use stochastic gradient descent to make a movie map for the MovieLens 100k data. Plot the map of the movies when you are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for question 8 here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
